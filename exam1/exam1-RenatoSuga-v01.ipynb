{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c425b9a1-862f-48fc-9472-32e44b023521",
   "metadata": {},
   "source": [
    "# Examen 1 : Cours A59 Apprentissage par renforcement\n",
    "<br>\n",
    "Nous allons lors de cet examen tester l'utilisation de Monte Carlo avec l'environnement CliffWalking.<br>\n",
    "\n",
    "L'objectif sera d'appliquer Monte Carlo first visit et de vérifier que l'algorithme est capable de résoudre le problème.<br>\n",
    "\n",
    "### Le plan est le suivant\n",
    "1) Découvrir l'environnement et répondre à quelques questions théoriques ( 9 pts)<br>\n",
    "2) Définir une politique (2pts)\n",
    "3) Lancer la simulation et en tirer les conclusions (10 pts)\n",
    "4) Ajouter epsilon à travers les itérations (4 pts)\n",
    "\n",
    "<br>\n",
    "Consignes:<br>\n",
    "\n",
    "Vous devrez compléter le code la ou est présente la mention __# votre code__ <br>\n",
    "Vous pouvez choisir d'implémenter différemment l'algortihme tant que vous utilisez Monte Carlo.<br>\n",
    "\n",
    "Vous devrez également répondre aux questions là ou vous trouverez la mention __# votre réponse__<br>\n",
    "\n",
    "Vous avez 3h00.<br>\n",
    "\n",
    "Le total est de 25 points.<br>\n",
    "\n",
    "__Merci de m'envoyer le notebook par MIO et en renommant le fichier avec votre nom__<br>\n",
    "\n",
    "C'est un travail personnel.<br>\n",
    "\n",
    "Les documents et TP sont permis.<br>\n",
    "<br>\n",
    "## Bonne chance !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f9d60-4319-4211-b664-6ec80f131726",
   "metadata": {},
   "source": [
    "# Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911295f3-6a68-4087-bce2-812839943ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f111df-2c6a-415b-888a-7074a6134b53",
   "metadata": {},
   "source": [
    "# Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c4f569-a7c6-4977-a841-2003547e78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATION = 5000\n",
    "NUM_STEP=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5cccd8-eb52-4532-8a62-54db6245906a",
   "metadata": {},
   "source": [
    "## Cliffwalking\n",
    "\n",
    "Le détail de l'environnement est accessible ici : https://gymnasium.farama.org/environments/toy_text/cliff_walking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbcc8df-f197-4e04-a00b-08fbe106b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0', render_mode=\"ansi\") \n",
    "\n",
    "# env_test sera utilisé plus tard pour visualier le résultat\n",
    "env_test = gym.make('CliffWalking-v0', render_mode=\"human\") # ,render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1949a8-f8e2-44f2-ac5f-1eed14df5229",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737eb3e-8ba9-4c2c-8f45-767ddc7f2e16",
   "metadata": {},
   "source": [
    "### (1) Récupérer le nombre d'états possibles à l'aide la classe env et affecter cette valeur à la variable nombre_etat (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22fd827f-d15a-4408-a42c-7404c79623ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# nombre d'états\n",
    "nombre_etat = env.observation_space.n\n",
    "print(nombre_etat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d005c820-7451-4dc6-a3ba-ad27010ac748",
   "metadata": {},
   "source": [
    "### __Question : combien avons nous d'état ?__<br>\n",
    "__Votre réponse :__"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c3d7811-87de-4fa9-bce3-cc85a075f70e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff000f-b622-45dd-ab2b-5e4093ced65b",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc0507-1239-485f-88ff-275ef02b1908",
   "metadata": {},
   "source": [
    "### (2) Récupérer le nombre d'action possible à l'aide la classe env et affecter cette valeur à la variable nombre_action (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d917c30-b95b-4153-ad37-620e7b6e0ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# nombre d'actions possibles\n",
    "\n",
    "nombre_action = env.action_space.n\n",
    "print(nombre_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a843619-0340-4482-ba53-c4eacdf61800",
   "metadata": {},
   "source": [
    "### __Question : combien avons nous d'action ?__<br>\n",
    "__Votre réponse :__"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9327f4ce-9d7e-44f3-ab40-402d29356ddb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ee393-7273-4555-87a7-3d714a903777",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e388b221-95b8-4b86-b659-62a64a2824a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La ligne suivante ne devrait pas générer d'erreur\n",
    "assert nombre_action+nombre_etat == 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b96ad1-ea9d-4134-93fb-95b94d8e5938",
   "metadata": {},
   "source": [
    "### (3) Question : est ce un environnement épisodique ou continu ? (1 pts)\n",
    "Votre réponse :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d31e9c1-d0ef-4f67-8760-d722c1149e54",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "Episodique. Les états sont bien définis et il y a une fin bien définie (quand on arrive à la case 47, l'objectif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2c518-c132-4026-a968-3a85e0913b20",
   "metadata": {},
   "source": [
    "### (4) Question : est ce un environnement conforme aux critéres d'un processus de décision Markovien et pourquoi ? (3 pts)\n",
    "Votre réponse :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8693b8a5-ee1e-41ff-96f3-cbfc49a92ab3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "Oui puisque l'action suivante ne dépend pas de l'historique, elle dépend uniquement de l'état présent.\n",
    "Il y a des récompenses sont définies en fonction de chaque transition d'état"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ea3865-fb63-4df6-bb62-da93df21ea74",
   "metadata": {},
   "source": [
    "### (5) Question : Pouvez vous décrire en quelques lignes le principe de l'algorithme Monte Carlo ? (1.5 pts)\n",
    "Votre réponse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b25267-30ae-4637-935e-f1b48674a5cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (3983922663.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    L'agorlithme Monte Carlo est indépendant du modèle (ça veut dire, on ne connaît pas la dynamique de transitions)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "L'agorlithme Monte Carlo est indépendant du modèle (ça veut dire, on ne connaît pas la dynamique de transitions)\n",
    "Pour arriver à une bonne politique, l'algorithme s'améliore à chaque tour (episode). Il fonctionne par échantillonage\n",
    "Plus que l'on joue, plus que l'on améliore la politque et on fini par convergir vers une politique optimale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01d022-6e4a-40ab-b695-8d4fb5be5eee",
   "metadata": {},
   "source": [
    "### (6) Question : Quelles sont les situations ou un algorithme Monte Carlo ne fonctionnera pas ? (1.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef47fa-ca4a-4316-aa74-9cd72615a4a7",
   "metadata": {},
   "source": [
    "- Environnement non-episodique\n",
    "- Si on ne peut pas faire assez d'échantillonage\n",
    "- Environnement non-markovien\n",
    "- La fin d'une episode est très long: le calcul de la récompense dépend de la fin de l'episode. \n",
    "Cela peut retarder le calcul de recompenses et, par conséquent, l'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320c5af-b5bb-4b96-a13b-c561647b3ca1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d7ee1-b14a-49b5-820e-052921b205e2",
   "metadata": {},
   "source": [
    "# (7) Fonctions utiles ( 2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c16572e-c0df-464e-8ad1-03e77ab9b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state,Q,epsilon):\n",
    "  \n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample() # exploration\n",
    "    else:\n",
    "        max_q=-1\n",
    "        max_a=0\n",
    "        for a in range(env.action_space.n):\n",
    "            if (state,a) in Q:\n",
    "                if Q[(state,a)] > max_q:\n",
    "                    max_q=Q[(state,a)]\n",
    "                    max_a=a\n",
    "        return(max_a)\n",
    "        # return max(list(range(env.action_space.n)), key = lambda x: Q[(state,x)]) # exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb833b-b275-4c27-806f-a28eafc41024",
   "metadata": {},
   "source": [
    "### ne devrait pas générer d'erreur mais cela peut varier avec votre version.\n",
    "Donc donner juste à titre indicatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ec8b95-142c-4019-b618-de045c218ea8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'int' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# valable si vous avez utilisé numpy pour générer les nombres aléatoires\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mpolicy\u001b[0;34m(state, Q, epsilon)\u001b[0m\n\u001b[1;32m      7\u001b[0m max_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m Q[(state,a)] \u001b[38;5;241m>\u001b[39m max_q:\n\u001b[1;32m     11\u001b[0m             max_q\u001b[38;5;241m=\u001b[39mQ[(state,a)]\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
     ]
    }
   ],
   "source": [
    "np.random.seed(1) # valable si vous avez utilisé numpy pour générer les nombres aléatoires\n",
    "assert policy([0,1,5,2],0,0) == 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83e6ae3e-8e4d-477c-8570-64a476987e15",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# valable si vous avez utilisé numpy pour générer les nombres aléatoires\u001b[39;00m\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m policy([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m2\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(1) # valable si vous avez utilisé numpy pour générer les nombres aléatoires\n",
    "env.action_space.seed(1)\n",
    "assert policy([0,1,5,2],1,0)==1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8858fb4-05f0-4af4-9816-2e459aa5417a",
   "metadata": {},
   "source": [
    "### Pas de modification nécessaire pour cette fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18041150-8a97-488b-815b-a2557722b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(Q,epsilon,num_timesteps):\n",
    "    \n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "\n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        # Sélection d'une action en fonction de notre politique\n",
    "        action = policy(Q,epsilon,state)\n",
    "        \n",
    "        # envoie de l'action à l'environnement pour retour (s_, r, done)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # stockage dans la liste du triplet (état, action, récompense)\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95856eee-97a3-44d4-8d78-50c829c147c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005cae4f-ff37-49d8-9fdc-2e2d1db4a9cd",
   "metadata": {},
   "source": [
    "# Lancons la simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9434b520-06db-467e-a064-c71cd5858531",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Définissons nos 3 tableaux numpy avec 2 dimensions état/action:<br>\n",
    "__Q__ : fonction d'action valeur donc indexé sur les états et les actions par état. Attention à initialiser avec 0.0<br>\n",
    "__total_return__ :  cumul par couple état/action des récompenses<br>\n",
    "__N__ : comptage du nombre de passage par couple état/action choisie<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986afda-691f-482f-8417-c8a2bfab1acb",
   "metadata": {},
   "source": [
    "### début repère 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7a762-de7d-41f4-92c7-10cca1a76a15",
   "metadata": {},
   "source": [
    "### (8) Créer et initialiser les tableaux (1 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc635f54-dfff-4c4f-9880-7e44774169e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "Q = np.zeros((nombre_etat, nombre_action))\n",
    "total_return = ((nombre_etat, nombre_action))\n",
    "N = ((nombre_etat, nombre_action))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e8702-934b-4559-bfe0-71e2d521fdd2",
   "metadata": {},
   "source": [
    "### (9) écrivez le code manquant Monte Carlo __First visit__  (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50212617-3dfa-4637-848b-d6658764e9f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m historique_recompense \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m     12\u001b[0m     \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# on génére un épisode\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     episode \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# on stocker les pairs s,a de l'épisode\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     all_state_action_pairs \u001b[38;5;241m=\u001b[39m [(s, a) \u001b[38;5;28;01mfor\u001b[39;00m (s,a,r) \u001b[38;5;129;01min\u001b[39;00m episode]\n",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m, in \u001b[0;36mgenerate_episode\u001b[0;34m(Q, epsilon, num_timesteps)\u001b[0m\n\u001b[1;32m     10\u001b[0m action \u001b[38;5;241m=\u001b[39m policy(Q,epsilon,state)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# envoie de l'action à l'environnement pour retour (s_, r, done)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m next_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# stockage dans la liste du triplet (état, action, récompense)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m episode\u001b[38;5;241m.\u001b[39mappend((state, action, reward))\n",
      "File \u001b[0;32m~/PythonVenvs/venv_py12/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PythonVenvs/venv_py12/lib/python3.12/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PythonVenvs/venv_py12/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PythonVenvs/venv_py12/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:195\u001b[0m, in \u001b[0;36mCliffWalkingEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[0;32m--> 195\u001b[0m     transitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    196\u001b[0m     i \u001b[38;5;241m=\u001b[39m categorical_sample([t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transitions], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random)\n\u001b[1;32m    197\u001b[0m     p, s, r, t \u001b[38;5;241m=\u001b[39m transitions[i]\n",
      "\u001b[0;31mKeyError\u001b[0m: 11"
     ]
    }
   ],
   "source": [
    "# condition FIXES du test\n",
    "np.random.seed(1)\n",
    "num_iterations=NUM_ITERATION\n",
    "num_step=NUM_STEP\n",
    "epsilon=0.20\n",
    "gamma=1 # doit faciliter le calcul de G\n",
    "\n",
    "# expérimentation\n",
    "historique_duree_partie = []\n",
    "historique_recompense = []\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # on génére un épisode\n",
    "    episode = generate_episode(Q,epsilon,num_step)\n",
    "    \n",
    "    # on stocker les pairs s,a de l'épisode\n",
    "    all_state_action_pairs = [(s, a) for (s,a,r) in episode]\n",
    "    \n",
    "    # on stocke les récompense\n",
    "    rewards = [r for (s,a,r) in episode]\n",
    "\n",
    "    historique_duree_partie.append(len(episode))\n",
    "    historique_recompense.append(np.sum(rewards))\n",
    "\n",
    "    # en appliquant l'algorithme de Monte Carlo première visite, metter à jour total_return, N et par calcul Q\n",
    "    # votre code (environ 6 lignes) ( 5pts)\n",
    "    # Calculer le retour total G pour chaque état-action\n",
    "    G = 0\n",
    "    # On parcourt l'épisode à l'envers pour calculer le retour G\n",
    "    for t in reversed(range(len(episode))):\n",
    "        G = gamma * G + rewards[t]  # Mettre à jour le retour total\n",
    "        state_action_pair = (all_state_action_pairs[t])\n",
    "        \n",
    "        # Si c'est la première visite de l'état-action\n",
    "        if state_action_pair not in all_state_action_pairs[:t]:\n",
    "            # Mettre à jour Q(s, a) et le nombre de visites\n",
    "            if state_action_pair not in Q:\n",
    "                Q[state_action_pair] = 0  # Initialiser si non présent\n",
    "            # Mise à jour de Q(s, a) avec le retour total G\n",
    "            Q[state_action_pair] += (G - Q[state_action_pair]) / (N[state_action_pair] + 1)\n",
    "            N[state_action_pair] += 1  # Incrémenter le compteur de visites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f735342-2075-44d1-8f73-c55ec380574a",
   "metadata": {},
   "source": [
    "---\n",
    "### Affichons l'évolution de la durée des épisodes au fil de l'expérimentation et l'historique des récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd0b9a-52bf-40e9-9108-3b8a8d402fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f ,(ax1,ax2) = plt.subplots(2,1,sharex=True)\n",
    "\n",
    "ax1.set_title(\"Durée des épisodes\")\n",
    "ax1.plot(historique_duree_partie)\n",
    "\n",
    "ax2.set_title(\"Récompense\")\n",
    "ax2.plot(historique_recompense)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da792a7-5735-4c51-8a3e-fa7db810dc5d",
   "metadata": {},
   "source": [
    "### fin repère 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a497c-1c8a-4e1f-83f0-93e97628dca5",
   "metadata": {},
   "source": [
    "## Vous devriez observer des courbes proches de celles ci dessous\n",
    "\n",
    "![courbes](static/courbes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0af83-36f0-4fb8-b685-1e3f6b35b800",
   "metadata": {},
   "source": [
    "### (10) Question : quelle est l'action recommandée par notre politique en position 36 ? Est ce correct ? (1 pt)\n",
    "Votre réponse :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbe5b461-6e99-4de2-b735-decf56ff2789",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4caf00-f7b0-4d14-9736-56f537001cef",
   "metadata": {},
   "source": [
    "### (11) Question : quelle est l'action recommandée par notre politique en position 35 ? Est ce correct ? (1 pt)\n",
    "Votre réponse :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d407da33-98d5-4349-be29-d98a84ef9b91",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd7bc0-7333-4827-9770-e4cd8c3c5b07",
   "metadata": {},
   "source": [
    "### Affichons une simulation d'un épisode.\n",
    "Note : si vous avez des difficultés à la version graphique, vous pouvez changer le mode render_mode=\"ansi\" pour env_test et ajouter un print(env.render()) dans la boucle\n",
    "\n",
    "#### Note il pourrait arriver que l'agent se bloque dans une colonne de la case gauche. Cela ne veut pas dire que votre algorithme n'a pas fonctionné mais nécessite plus d'entrainement.\n",
    "\n",
    "__Solution : refaite un entrainement à partir de la section début repère 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883c21e-c2cd-4e7d-8080-50c6bfccb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env_test.reset()[0]\n",
    "done=False\n",
    "while not done:\n",
    "    action=policy(Q,0,state)\n",
    "    state,_,done,truncated,_ = env_test.step(action)\n",
    "    # print(env.render())\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ec106-a41d-4c09-9a8b-3ac5be169acd",
   "metadata": {},
   "source": [
    "### (12) Question : Le petit homme prend il le chemin le plus court et pourquoi ? (2.0 pt)\n",
    "Votre réponse :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38366fe0-a376-4f2e-9d1a-7a7bf4c701a4",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fadc073-d46d-4204-a7ff-a411253fd6eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af486954-4d23-419d-9ae3-0dc46bc1ca5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a9f3d-9113-4e13-a8c3-85630b8d038c",
   "metadata": {},
   "source": [
    "## Copier le code compris entre les balises **début repère 1** et **fin repère 1** ci dessous.\n",
    "## Notre approche e-greedy n'est pas complétement satisfaisante.\n",
    "## Nous allons faire une décroissance d'espilon progressive.\n",
    "### (13) Modifier le code pour faire évoluer epsilon de 0.90 à 0.05 selon un nombre d'éspisode que je vous laisse apprécier et relancer l'expérimentation. (3 pts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1fc03-8793-4801-be2d-54c76d2c73f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f28107-4233-409c-99bd-9c7c584537ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54be24c-52c5-429f-a2ce-39dffc63dbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7105e2c-926c-4ec8-9653-85dcc942b290",
   "metadata": {},
   "source": [
    "Vous devriez obtenir des courbes prochent de celles ci (peut varier)\n",
    "\n",
    "![courbes](static/courbes_decay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486f8a6-0ab1-4b8d-a8aa-eb8d24e3d730",
   "metadata": {},
   "source": [
    "### (14) Question : pourquoi la partie gauche des courbes est elle plus épaisse qu'avec une valeur d'epsilon fixe ? (1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0e7b0-b022-4dbb-9393-bf1cdb009f4c",
   "metadata": {},
   "source": [
    "votre résponse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9dc1e-2054-4b37-b13c-77f116db6c99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b037d1-459e-45b5-a9f6-373686d471f9",
   "metadata": {},
   "source": [
    "# Ne pas oublier de m'envoyer le notebook par MIO !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
