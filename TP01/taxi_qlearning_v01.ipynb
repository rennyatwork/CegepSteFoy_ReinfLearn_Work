{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9doYB31g8huAxBYWYus3j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rennyatwork/CegepSteFoy_ReinfLearn_Work/blob/main/TP01/taxi_qlearning_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "x0PU0YFT0YHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXf87_NPzzOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec72e70-b76e-48ad-e483-60b176393a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 10000, Average reward (last 10000 episodes): -0.9844249134782082, current epsilon: 0.15\n",
            "Episode: 20000, Average reward (last 10000 episodes): -0.4936645309625605, current epsilon: 0.15\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "\n",
        "def create_q_table(env):\n",
        "  \"\"\"Creates a Q-table with all values initialized to 0.\n",
        "\n",
        "  Args:\n",
        "    env: The Gymnasium environment.\n",
        "\n",
        "  Returns:\n",
        "    A NumPy array representing the Q-table.\n",
        "  \"\"\"\n",
        "  return np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "def plot_reward_evolution(rewards\n",
        "                          , pXLabel = 'Episode'\n",
        "                          , pYLabel = 'Reward'\n",
        "                          , pTitle='Reward Evolution during Training'):\n",
        "  \"\"\"Plots the evolution of rewards over episodes.\n",
        "\n",
        "  Args:\n",
        "    rewards: A list of rewards obtained during training.\n",
        "  \"\"\"\n",
        "  plt.clf() #clear cell\n",
        "  plt.plot(rewards)\n",
        "  plt.xlabel(pXLabel)\n",
        "  plt.ylabel(pYLabel)\n",
        "  plt.title(pTitle)\n",
        "\n",
        "  # Calculate tick positions for 10 evenly spaced labels\n",
        "  num_ticks = 10\n",
        "  x_min = 0  # Assume x-axis starts at 0\n",
        "  x_max = len(rewards) - 1  # Assume x-axis ends at the last data point\n",
        "  tick_positions = np.linspace(x_min, x_max, num_ticks, dtype=int)\n",
        "\n",
        "   # Set x-axis ticks and labels\n",
        "  plt.xticks(tick_positions, tick_positions)  # Set both positions and labels\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def get_policy(q_table, state, epsilon):\n",
        "  \"\"\"Selects an action using the epsilon-greedy policy.\n",
        "\n",
        "  Args:\n",
        "    q_table: The Q-table containing learned values.\n",
        "    state: The current state of the environment.\n",
        "    epsilon: The exploration rate.\n",
        "\n",
        "  Returns:\n",
        "    The action selected by the policy.\n",
        "  \"\"\"\n",
        "  if np.random.uniform(0, 1) < epsilon:\n",
        "    action = env.action_space.sample()  # Explore action space\n",
        "  else:\n",
        "    action = np.argmax(q_table[state])  # Exploit learned values\n",
        "  return action\n",
        "\n",
        "def train_q_learning(env\n",
        "                     , q_table\n",
        "                     , alpha=0.1\n",
        "                     , gamma=0.9\n",
        "                     , epsilon=1.0\n",
        "                     , episodes=100000\n",
        "                     , pPrint=False\n",
        "                     ):\n",
        "  \"\"\"Trains the Q-learning agent.\n",
        "\n",
        "  Args:\n",
        "    env: The Gymnasium environment.\n",
        "    q_table: The Q-table to update.\n",
        "    alpha: The learning rate.\n",
        "    gamma: The discount factor.\n",
        "    epsilon: The exploration rate.\n",
        "    episodes: The number of training episodes.\n",
        "  \"\"\"\n",
        "\n",
        "  rewards = []\n",
        "  all_rewards_per_episode = [] # List to store rewards per episode\n",
        "  epsilon_decay = 0.9999  # Example decay rate\n",
        "  start_time_train = time.time()\n",
        "\n",
        "  for i in range(1, episodes + 1):\n",
        "    state = env.reset()[0]\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    done = False\n",
        "    start_time_for = time.time()\n",
        "\n",
        "\n",
        "    while not done:\n",
        "\n",
        "      action = get_policy(q_table, state, epsilon)\n",
        "\n",
        "      # Decay epsilon over time:\n",
        "      epsilon *= epsilon_decay\n",
        "      epsilon = max(epsilon, 0.05) # set the min epsilon\n",
        "\n",
        "      next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "      old_value = q_table[state, action]\n",
        "      next_max = np.max(q_table[next_state])\n",
        "\n",
        "      new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "      q_table[state, action] = new_value\n",
        "\n",
        "      if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "      state = next_state\n",
        "      epochs += 1\n",
        "\n",
        "      rewards.append(reward)\n",
        "\n",
        "\n",
        "    end_time_for = time.time()\n",
        "    elapsed_time_for = start_time_for - end_time_for\n",
        "\n",
        "    if pPrint:\n",
        "      print(\"****************************************************\")\n",
        "      print(f\"Episode: {i}\")\n",
        "      print(f\"Timesteps taken: {epochs}\")\n",
        "      print(f\"Penalties incurred: {penalties}\")\n",
        "\n",
        "\n",
        "    all_rewards_per_episode.append(np.mean(rewards)) # Calculating avg. reward per episode\n",
        "\n",
        "    ten_pct_print = int(episodes/10)\n",
        "\n",
        "    if i % (ten_pct_print) == 0:\n",
        "      #print(f\"Episode: {i}\")\n",
        "      avg_reward_last_100 = np.mean(all_rewards_per_episode[-ten_pct_print:]) # Calculating avg. reward for the last 100 episodes\n",
        "      #print(f\"Episode: {i}, Avg reward (last {ten_pct_print} episodes): {avg_reward_last_100}, current epsilon: {epsilon}, Elapsed time current for: {elapsed_time_for} secon\")\n",
        "      print(f\"Episode: {i}, \"\n",
        "      f\"Avg reward (last {ten_pct_print} episodes): \"\n",
        "      f\"{avg_reward_last_100}, \"\n",
        "      f\"current epsilon: {epsilon}, \"\n",
        "      f\"Elapsed time current for: {elapsed_time_for} seconds\")\n",
        "\n",
        "\n",
        "    #if i % 1000 == 0:\n",
        "     # plot_reward_evolution(rewards)\n",
        "\n",
        "  end_time_train = time.time()\n",
        "  elapsed_time_train = end_time_train - start_time_train\n",
        "  print(f\"Training time: {elapsed_time_train} seconds\")\n",
        "  print(\"Training finished.\\n\")\n",
        "\n",
        "def evaluate_agent(env, q_table, episodes=100, pPrint=False):\n",
        "  \"\"\"Evaluates the trained agent.\n",
        "\n",
        "  Args:\n",
        "    env: The Gymnasium environment.\n",
        "    q_table: The trained Q-table.\n",
        "    episodes: The number of evaluation episodes.\n",
        "\n",
        "  Returns:\n",
        "    The average reward over the evaluation episodes.\n",
        "  \"\"\"\n",
        "\n",
        "  if pPrint:\n",
        "      print('Evaluating agent...')\n",
        "\n",
        "  arr_avg_rewards = []\n",
        "  arr_total_rewards = []\n",
        "\n",
        "  total_rewards = 0\n",
        "  for ep in range(episodes):\n",
        "    state = env.reset()[0]\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "\n",
        "    steps_so_far = 0\n",
        "\n",
        "\n",
        "    while not done and steps_so_far < 300:\n",
        "      action = np.argmax(q_table[state])\n",
        "      state, reward, done, truncated, info = env.step(action)\n",
        "      steps_so_far +=1\n",
        "\n",
        "      episode_reward += reward\n",
        "\n",
        "      #if reward == -10:\n",
        "      #  penalties += 1\n",
        "\n",
        "      reward_per_step = episode_reward / steps_so_far\n",
        "\n",
        "\n",
        "      if (pPrint and (steps_so_far % 50) ==0):\n",
        "        print(f'steps_so_far: {steps_so_far}')\n",
        "        print(f'Reward/step in episode [{ep}]: {reward_per_step}')\n",
        "\n",
        "    total_rewards += episode_reward\n",
        "\n",
        "    if pPrint:\n",
        "      print(f\"Episode [{ep}] reward: {episode_reward}\")\n",
        "\n",
        "    arr_avg_rewards.append(reward_per_step)\n",
        "    arr_total_rewards.append(episode_reward)\n",
        "\n",
        "  print(f\"Results after {episodes} episodes:\")\n",
        "  print(f'len(arr_avg_rewards): {len(arr_avg_rewards)} ')\n",
        "  print(f'len(arr_total_rewards): {len(arr_total_rewards)} ')\n",
        "\n",
        "\n",
        "  plot_reward_evolution(arr_avg_rewards\n",
        "                        , pYLabel='Average Reward'\n",
        "                        , pTitle='Average Reward evolution')\n",
        "  plot_reward_evolution(arr_total_rewards\n",
        "                        , pYLabel='Total Rewards'\n",
        "                        , pTitle='Total Rewards evolution')\n",
        "\n",
        "  return total_rewards / episodes\n",
        "\n",
        "# Main execution\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "q_table = create_q_table(env)\n",
        "train_q_learning(env, q_table\n",
        "                 , epsilon=1.0\n",
        "                 , episodes=100)\n",
        "average_reward = evaluate_agent(env\n",
        "                                , q_table\n",
        "                                , episodes=200\n",
        "                                ,  pPrint=True)\n",
        "\n",
        "print(f\"Average reward: {average_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ym1pQspg0Q4x"
      }
    }
  ]
}