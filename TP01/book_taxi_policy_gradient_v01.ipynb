{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rennyatwork/CegepSteFoy_ReinfLearn_Work/blob/main/TP01/book_taxi_policy_gradient_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf33WlF4FiLh"
      },
      "source": [
        "# Cart pole balancing with policy gradient\n",
        "\n",
        "Now, let's learn how to implement the policy gradient algorithm with reward-to-go for the\n",
        "cart pole balancing task.\n",
        "\n",
        "First, let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRRMIRe1FkgX",
        "outputId": "f780a309-e778-4b74-adaa-270d67154636"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pGmxf2WFiLj",
        "outputId": "60b1b1a1-350a-4291-8e46-7b0ee0379ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UrWUauduFiLl"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnm391aHFiLl"
      },
      "source": [
        "For a clear understanding of how the policy gradient method works, we use\n",
        "TensorFlow in the non-eager mode by disabling TensorFlow 2 behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnyL77DOFiLl",
        "outputId": "8cb756b9-2d3c-44a7-9b52-d2c821ae496e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4K5Cmj7FiLl"
      },
      "source": [
        "Create the cart pole environment using gym:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Zfz_2jWgFiLm"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbev3c7rFiLm"
      },
      "source": [
        "Get the state shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x9U0Q7oPFiLm"
      },
      "outputs": [],
      "source": [
        "state_shape = env.observation_space.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBurAWe7FiLm"
      },
      "source": [
        "Get the number of actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VmAZh4_MFiLm"
      },
      "outputs": [],
      "source": [
        "num_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40_R2HqMFiLn"
      },
      "source": [
        "## Computing discounted and normalized reward\n",
        "\n",
        "Instead of using the rewards directly, we can use the discounted and normalized rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGaBh5wwFiLn"
      },
      "source": [
        "Define the discount factor, $\\gamma$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yYh8nLC6FiLn"
      },
      "outputs": [],
      "source": [
        "gamma = 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVokDFivFiLn"
      },
      "source": [
        "Let's define a function called `discount_and_normalize_rewards` for computing the discounted and normalized rewards:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kzzZg5IKFiLn"
      },
      "outputs": [],
      "source": [
        "def discount_and_normalize_rewards(episode_rewards):\n",
        "\n",
        "    #initialize an array for storing the discounted reward\n",
        "    discounted_rewards = np.zeros_like(episode_rewards)\n",
        "\n",
        "    #compute the discounted reward\n",
        "    reward_to_go = 0.0\n",
        "    for i in reversed(range(len(episode_rewards))):\n",
        "        reward_to_go = reward_to_go * gamma + episode_rewards[i]\n",
        "        discounted_rewards[i] = reward_to_go\n",
        "\n",
        "    #normalize and return the reward\n",
        "    discounted_rewards -= np.mean(discounted_rewards)\n",
        "    discounted_rewards /= np.std(discounted_rewards)\n",
        "\n",
        "    return discounted_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QykNmFPOFiLo"
      },
      "source": [
        "## Building the policy network\n",
        "\n",
        "First, let's define the placeholder for the state:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "A9K_VQZPFiLo",
        "outputId": "57ba50f5-bdcb-4a00-d6c8-99bb18de4773"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got value 'Discrete(500)' with type '<class 'gym.spaces.discrete.Discrete'>'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m   1548\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1549\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    743\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0;34m\"Dimension value must be integer or None or have \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got value 'Discrete(500)' with type '<class 'gym.spaces.discrete.Discrete'>'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-58c53740ec2e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_shape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"state_ph\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   2992\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   2993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2994\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   7086\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7087\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7088\u001b[0;31m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7089\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m   7090\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" %\n",
            "\u001b[0;31mTypeError\u001b[0m: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got value 'Discrete(500)' with type '<class 'gym.spaces.discrete.Discrete'>'."
          ]
        }
      ],
      "source": [
        "state_ph = tf.placeholder(tf.float32, [None, state_shape], name=\"state_ph\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXtVVUsSFiLo"
      },
      "source": [
        "Define the placeholder for the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjQilQOyFiLo"
      },
      "outputs": [],
      "source": [
        "action_ph = tf.placeholder(tf.int32, [None, num_actions], name=\"action_ph\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Tf4_8wFiLo"
      },
      "source": [
        "Define the placeholder for the discounted reward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udII-mkxFiLo"
      },
      "outputs": [],
      "source": [
        "discounted_rewards_ph = tf.placeholder(tf.float32, [None,], name=\"discounted_rewards\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yotGztcrFiLp"
      },
      "source": [
        "Define the layer 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXtejS6fFiLp"
      },
      "outputs": [],
      "source": [
        "layer1 = tf.layers.dense(state_ph, units=32, activation=tf.nn.relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lOR7C-rFiLp"
      },
      "source": [
        "Define the layer 2, note that the number of units in the layer 2 is set to the number of\n",
        "actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trXdAolFFiLp"
      },
      "outputs": [],
      "source": [
        "layer2 = tf.layers.dense(layer1, units=num_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uJv7FYhFiLp"
      },
      "source": [
        "Obtain the probability distribution over the action space as an output of the network by\n",
        "applying the softmax function to the result of layer 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSto115tFiLp"
      },
      "outputs": [],
      "source": [
        "prob_dist = tf.nn.softmax(layer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U66NHvRJFiLq"
      },
      "source": [
        "We learned that we compute gradient as:\n",
        "\n",
        "$$\\nabla_{\\theta} J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N}\\left[\\sum_{t=0}^{T-1}  \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} | s_{t}\\right)R_t\\right] $$\n",
        "    \n",
        "After computing the gradient we update the parameter of the network using the gradient\n",
        "ascent as:    \n",
        "\n",
        "$$\\theta = \\theta + \\alpha \\nabla_{\\theta} J(\\theta) $$\n",
        "\n",
        "However, it is a standard convention to perform minimization rather than maximization.\n",
        "So, we can convert the above maximization objective into the minimization objective by just\n",
        "adding a negative sign.\n",
        "\n",
        "Thus, we can define negative log policy as:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNvzlbAbFiLq"
      },
      "outputs": [],
      "source": [
        "neg_log_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = layer2, labels = action_ph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4YB4W-tFiLq"
      },
      "source": [
        "Now, let's define the loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpANKJgPFiLq"
      },
      "outputs": [],
      "source": [
        "loss = tf.reduce_mean(neg_log_policy * discounted_rewards_ph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4nGO8ayFiLq"
      },
      "source": [
        "Define the train operation for minimizing the loss using Adam optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQk44E-XFiLr"
      },
      "outputs": [],
      "source": [
        "train = tf.train.AdamOptimizer(0.01).minimize(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9A7RvLRFiLr"
      },
      "source": [
        "## Training the network\n",
        "\n",
        "Now, let's train the network for several iterations. For simplicity, let's just generate one\n",
        "episode on every iteration.\n",
        "\n",
        "Set the number of iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6WzliGkFiLr"
      },
      "outputs": [],
      "source": [
        "num_iterations = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnzU8sqaFiLr",
        "outputId": "2fe4e4bd-5d13-41bc-8e10-8f777b688a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration:0, Return: 71.0\n",
            "Iteration:10, Return: 12.0\n"
          ]
        }
      ],
      "source": [
        "#start the TensorFlow session\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    #initialize all the TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    #for every iteration\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        #initialize an empty list for storing the states, actions, and rewards obtained in the episode\n",
        "        episode_states, episode_actions, episode_rewards = [],[],[]\n",
        "\n",
        "        #set the done to False\n",
        "        done = False\n",
        "\n",
        "        #initialize the state by resetting the environment\n",
        "        state = env.reset()\n",
        "\n",
        "        #initialize the return\n",
        "        Return = 0\n",
        "\n",
        "        #while the episode is not over\n",
        "        while not done:\n",
        "\n",
        "            #reshape the state\n",
        "            state = state.reshape([1,4])\n",
        "\n",
        "            #feed the state to the policy network and the network returns the probability distribution\n",
        "            #over the action space as output which becomes our stochastic policy\n",
        "            pi = sess.run(prob_dist, feed_dict={state_ph: state})\n",
        "\n",
        "            #now, we select an action using this stochastic policy\n",
        "            a = np.random.choice(range(pi.shape[1]), p=pi.ravel())\n",
        "\n",
        "            #perform the selected action\n",
        "            next_state, reward, done, info = env.step(a)\n",
        "\n",
        "            #render the environment\n",
        "            env.render()\n",
        "\n",
        "            #update the return\n",
        "            Return += reward\n",
        "\n",
        "            #one-hot encode the action\n",
        "            action = np.zeros(num_actions)\n",
        "            action[a] = 1\n",
        "\n",
        "            #store the state, action, and reward into their respective list\n",
        "            episode_states.append(state)\n",
        "            episode_actions.append(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "            #update the state to the next state\n",
        "            state=next_state\n",
        "\n",
        "\n",
        "        #compute the discounted and normalized reward\n",
        "        discounted_rewards= discount_and_normalize_rewards(episode_rewards)\n",
        "\n",
        "        #define the feed dictionary\n",
        "        feed_dict = {state_ph: np.vstack(np.array(episode_states)),\n",
        "                     action_ph: np.vstack(np.array(episode_actions)),\n",
        "                     discounted_rewards_ph: discounted_rewards\n",
        "                    }\n",
        "\n",
        "        #train the network\n",
        "        loss_, _ = sess.run([loss, train], feed_dict=feed_dict)\n",
        "\n",
        "        #print the return for every 10 iteration\n",
        "        if i%10==0:\n",
        "            print(\"Iteration:{}, Return: {}\".format(i,Return))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AbGDKVsFiLs"
      },
      "source": [
        "Now that we have learned how to implement the policy gradient algorithm with rewardto-go, in the next section, we will learn another interesting variance reduction technique\n",
        "called policy gradient with baseline."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}