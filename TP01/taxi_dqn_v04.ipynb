{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rennyatwork/CegepSteFoy_ReinfLearn_Work/blob/main/TP01/taxi_dqn_v04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install segment_tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-QwruIujyHa",
        "outputId": "d7cd87d0-44f1-4d0d-97b8-489c99cbd984"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: segment_tree in /usr/local/lib/python3.10/dist-packages (0.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard libraries\n",
        "import random\n",
        "import time\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "# Gym (or Gymnasium, depending on the version)\n",
        "import gym  # For Gym, or use `import gymnasium as gym` if using Gymnasium specifically\n",
        "\n",
        "# TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "v3YBcCowbJMo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Environment Setup\n",
        "env = gym.make('Taxi-v3', render_mode=\"rgb_array\")\n",
        "state_size = env.observation_space.n  # Taxi-v3 has 500 states\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# DQN Model\n",
        "class DQNModel:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.model = self._build_model(state_size, action_size)\n",
        "        self.target_model = tf.keras.models.clone_model(self.model)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def _build_model(self, state_size, action_size):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(8, activation='relu', input_shape=(state_size,)),\n",
        "            tf.keras.layers.Dense(8, activation='relu'),\n",
        "            tf.keras.layers.Dense(action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      loss='mse')\n",
        "        return model\n",
        "\n",
        "    def predict(self, state, pPrint=False):\n",
        "        if pPrint:\n",
        "            print(f\"[predict] state: {state}\")\n",
        "        if np.isscalar(state):\n",
        "            state = np.eye(state_size)[state]  # Convert scalar to one-hot encoded array\n",
        "        return self.model.predict(np.array([state]), verbose=0)[0]\n",
        "\n",
        "    def update_target_model(self, pPrint=False):\n",
        "        if pPrint:\n",
        "            print(\"[update_target_model] Updating target network weights.\")\n",
        "        self.target_model.set_weights(self.model.get_weights())"
      ],
      "metadata": {
        "id": "T12mPo_9-XtC",
        "outputId": "5346f49b-9e49-411f-a19e-9dca31f53c82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LdkW6STLjnBL"
      },
      "outputs": [],
      "source": [
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 0.99   # exploration rate\n",
        "        self.epsilon_min = 0.005\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = DQNModel(state_size, action_size)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self, pPrint=False):\n",
        "        if pPrint:\n",
        "            print(\"[update_target_model] Calling model's update_target_model.\")\n",
        "        self.model.update_target_model()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done, pPrint=False):\n",
        "        if pPrint:\n",
        "            print(f\"[remember] State: {state}, Action: {action}, Reward: {reward}, Next_state: {next_state}, Done: {done}\")\n",
        "        state = np.eye(self.state_size)[state] if isinstance(state, int) else state\n",
        "        next_state = np.eye(self.state_size)[next_state] if isinstance(next_state, int) else next_state\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state, pPrint=False):\n",
        "        if pPrint:\n",
        "            print(\"[act] Epsilon-greedy action selection.\")\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values)\n",
        "\n",
        "    def replay(self, batch_size, pPrint=False):\n",
        "        if pPrint:\n",
        "            print('[replay]')\n",
        "\n",
        "        # Sample a minibatch from memory\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "\n",
        "        # Get all states and next states from the minibatch for batch processing\n",
        "        states = np.array([exp[0] for exp in minibatch])\n",
        "        next_states = np.array([exp[3] for exp in minibatch])\n",
        "\n",
        "        # Predict Q-values for all current states in the minibatch\n",
        "        q_values = self.model.model.predict(states, verbose=0)\n",
        "\n",
        "        # Predict Q-values for all next states using the target network\n",
        "        q_values_next = self.model.target_model.predict(next_states, verbose=0)\n",
        "\n",
        "        # Iterate over the minibatch and adjust Q-values\n",
        "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
        "            target = reward if done else reward + self.gamma * np.max(q_values_next[i])\n",
        "            q_values[i][action] = target\n",
        "\n",
        "        # Train the model on the batch of updated Q-values\n",
        "        self.model.model.fit(states, q_values, epochs=1, verbose=0)\n",
        "\n",
        "        # Decay epsilon for exploration\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, episodes, batch_size, pPrint=False, pModel=None, pPrintStats=True):\n",
        "\n",
        "        start_time_all = time.time()\n",
        "        total_score = 0\n",
        "\n",
        "        if pModel is not None:\n",
        "            self.model = pModel\n",
        "        else:\n",
        "            self.model = DQNModel(self.state_size, self.action_size)\n",
        "\n",
        "        if pPrint:\n",
        "            print(\"[BEGIN train] Starting training.\")\n",
        "        scores = []\n",
        "        for e in range(episodes):\n",
        "\n",
        "          if pPrint:\n",
        "            print(f'FOR loop, episode {e}')\n",
        "\n",
        "          start_time = time.time()\n",
        "          state = env.reset()  # Reset the environment correctly\n",
        "          done = False\n",
        "          score = 0\n",
        "          penalties = 0\n",
        "          iter = 0\n",
        "\n",
        "          while not done:\n",
        "\n",
        "            if pPrint:\n",
        "              print('')\n",
        "              print('------')\n",
        "              print(f'[while] - [episode]: {e},  [state]: {state},  [iter]: {iter}, [score]: {score},  [done]: {done}')\n",
        "\n",
        "            action = self.act(state, pPrint)  # Get action from the agent\n",
        "            next_state, reward, terminated, _ = env.step(action)\n",
        "\n",
        "            iter += 1\n",
        "            penalties += 1 if reward == -10 else 0\n",
        "\n",
        "            # Store the experience in memory\n",
        "            self.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            # Train the model if memory is sufficient\n",
        "            if len(self.memory) > batch_size:\n",
        "                self.replay(batch_size)\n",
        "\n",
        "            state = next_state\n",
        "            done = terminated\n",
        "\n",
        "            score += reward\n",
        "\n",
        "            ### Add additional conditions for early termination\n",
        "            if score < -20:\n",
        "              done = True\n",
        "              print('Too many negative points. Impossible to win. BYE!!!')\n",
        "            if iter >= 30:\n",
        "              done = True\n",
        "              print('Too many steps. Not going ANYWHERE. BYE!!!')\n",
        "            if penalties >= 2:\n",
        "              done = True\n",
        "              print('Too many wrong actions.  BYE!!!')\n",
        "\n",
        "            total_score += score\n",
        "            scores.append(score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          if pPrintStats:\n",
        "              print(f\"Episode {e + 1}/{episodes}, Score: {score}, total iter: {iter}, Avg Reward: {total_score / (e + 1):.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        elapsed_time = time.time() - start_time_all\n",
        "        print('----------------------')\n",
        "        print('[END training]')\n",
        "        print(f'Avg score  = {total_score / episodes:.2f}')\n",
        "        print(f\"Training complete in {elapsed_time:.2f} sec.\")\n",
        "\n",
        "        return scores, self.model  # Return scores and the trained model\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, env, episodes=5, pModel=None):\n",
        "        \"\"\"Evaluate the trained DQN agent by letting it play the game.\"\"\"\n",
        "        model_to_evaluate = pModel if pModel is not None else self.model\n",
        "        penalties = 0\n",
        "\n",
        "        for e in range(episodes):\n",
        "          #print(f'for loop, episode {e}')\n",
        "          start_time = time.time()\n",
        "          state = env.reset()\n",
        "          state = state[0] if isinstance(state, tuple) else state  # Handle Gym's tuple output\n",
        "          done = False\n",
        "          total_reward = 0\n",
        "          steps = 0\n",
        "\n",
        "          while not done:\n",
        "              # Use the trained model to select the best action\n",
        "              action = np.argmax(model_to_evaluate.predict(state))\n",
        "              next_state, reward, done, info = env.step(action)\n",
        "              total_reward += reward\n",
        "              steps += 1\n",
        "              state = next_state\n",
        "\n",
        "              if total_reward < -30:\n",
        "                  done = True\n",
        "                  print('Too many negative points. Impossible to win. BYE!!!')\n",
        "              if steps >= 30:\n",
        "                  done = True\n",
        "                  print('Too many steps. Not going ANYWHERE. BYE!!!')\n",
        "              if reward == -10:\n",
        "                  penalties += 1\n",
        "                  if penalties >= 2:\n",
        "                      done = True\n",
        "                      print('Too many wrong actions.  BYE!!!')\n",
        "\n",
        "          elapsed_time = time.time() - start_time  # Calculate elapsed time\n",
        "\n",
        "          print(f\"Episode {e + 1}: Total Reward = {total_reward}, Steps Taken = {steps}, Time Elapsed = {elapsed_time:.2f} seconds\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KoVjphyv7YCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "trained_agent = DQNAgent(state_size, action_size)  # Use existing DQNAgent class\n",
        "\n",
        "# Train the agent and get the trained model\n",
        "#scores, trained_model_01 = trained_agent.train(episodes=500, batch_size=32, pPrint=True)\n",
        "#scores, trained_model_02 = trained_agent.train(episodes=500, batch_size=32, pModel=trained_model_01, pPrint=True)\n",
        "\n",
        "\n",
        "scores, trained_model_01 = trained_agent.train(episodes=10, batch_size=32, pPrint=False)\n",
        "#scores, trained_model_02 = trained_agent.train(episodes=500, batch_size=32, pModel=trained_model_01, pPrint=False)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mIHE1ZZkFaJ",
        "outputId": "1e5c6cbe-b24d-45dc-ac2b-415784c2974e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Too many wrong actions.  BYE!!!\n",
            "Episode 1/10, Score: -22, total iter: 4, Avg Reward: -55.00, Time: 0.53s\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Too many wrong actions.  BYE!!!\n",
            "Episode 2/10, Score: -29, total iter: 11, Avg Reward: -105.50, Time: 0.13s\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 3/10, Score: -21, total iter: 12, Avg Reward: -99.33, Time: 0.27s\n",
            "Too many wrong actions.  BYE!!!\n",
            "Episode 4/10, Score: -20, total iter: 2, Avg Reward: -82.00, Time: 0.03s\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Too many wrong actions.  BYE!!!\n",
            "Episode 5/10, Score: -22, total iter: 4, Avg Reward: -73.00, Time: 1.41s\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Too many wrong actions.  BYE!!!\n",
            "Episode 6/10, Score: -27, total iter: 9, Avg Reward: -78.83, Time: 2.48s\n",
            "Too many wrong actions.  BYE!!!\n",
            "Episode 7/10, Score: -20, total iter: 2, Avg Reward: -71.86, Time: 0.49s\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Too many wrong actions.  BYE!!!\n",
            "Episode 8/10, Score: -29, total iter: 11, Avg Reward: -75.62, Time: 1.89s\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Too many wrong actions.  BYE!!!\n",
            "Episode 9/10, Score: -22, total iter: 4, Avg Reward: -73.33, Time: 0.70s\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 10/10, Score: -21, total iter: 12, Avg Reward: -75.60, Time: 2.15s\n",
            "----------------------\n",
            "[END training]\n",
            "Avg score  = -75.60\n",
            "Training complete in 10.14 sec.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "def train(self, episodes, batch_size, pPrint=False, pModel=None, pPrintStats=True):\n",
        "\n",
        "    start_time_all = time.time()\n",
        "    total_score = 0\n",
        "\n",
        "    if pModel is not None:\n",
        "        self.model = pModel\n",
        "    else:\n",
        "        self.model = DQNModel(self.state_size, self.action_size)\n",
        "\n",
        "    if pPrint:\n",
        "        print(\"[BEGIN train] Starting training.\")\n",
        "    scores = []\n",
        "    for e in range(episodes):\n",
        "\n",
        "        if pPrint:\n",
        "            print(f'FOR loop, episode {e}')\n",
        "\n",
        "        start_time = time.time()\n",
        "        state = env.reset()  # Reset the environment correctly\n",
        "        # Handle Gym's tuple output in reset()\n",
        "        state = state[0] if isinstance(state, tuple) else state\n",
        "        done = False\n",
        "        score = 0\n",
        "        penalties = 0\n",
        "        iter = 0\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            if pPrint:\n",
        "                print('')\n",
        "                print('------')\n",
        "                print(f'[while] - [episode]: {e},  [state]: {state},  [iter]: {iter}, [score]: {score},  [done]: {done}')\n",
        "\n",
        "            action = self.act(state, pPrint)  # Get action from the agent\n",
        "            # Update this line to handle additional return values from env.step()\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            # Alternatively, if you don't need truncated and info:\n",
        "            # next_state, reward, terminated, _, _ = env.step(action)\n",
        "\n",
        "            iter += 1\n",
        "            penalties += 1 if reward == -10 else 0\n",
        "\n",
        "            # Store the experience in memory\n",
        "            self.remember(state, action, reward, next_state, done)\n",
        "\n",
        "            # Train the model if memory is sufficient\n",
        "            if len(self.memory) > batch_size:\n",
        "                self.replay(batch_size)\n",
        "\n",
        "            state = next_state\n",
        "            done = terminated  # Update done based on 'terminated'\n",
        "\n",
        "            score += reward\n",
        "\n",
        "            ### Add additional conditions for early termination\n",
        "            if score < -20:\n",
        "                done = True\n",
        "                print('Too many negative points. Impossible to win. BYE!!!')\n",
        "            if iter >= 30:\n",
        "                done = True\n",
        "                print('Too many steps. Not going ANYWHERE. BYE!!!')\n",
        "            if penalties >= 2:\n",
        "                done = True\n",
        "                print('Too many wrong actions.  BYE!!!')\n",
        "\n",
        "            total_score += score\n",
        "            scores.append(score)\n",
        "\n",
        "\n",
        "\n",
        "        if pPrintStats:\n",
        "            print(f\"Episode {e + 1}/{episodes}, Score: {score}, total iter: {iter}, Avg Reward: {total_score / (e + 1):.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    elapsed"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "KXu3I8s5dI-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores, trained_model_02 = trained_agent.train(episodes=1500, batch_size=32, pModel=trained_model_01, pPrint=True)"
      ],
      "metadata": {
        "id": "3z9ZnsYN2kn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate using the trained model\n",
        "# Evaluate the trained model\n",
        "trained_agent.evaluate(env, episodes=10, pModel=trained_model_03)"
      ],
      "metadata": {
        "id": "eVAd-zbJe2lz",
        "outputId": "bd8c4156-560a-4367-9785-0b6c9fb8fcc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 1: Total Reward = -21, Steps Taken = 21, Time Elapsed = 2.21 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 2: Total Reward = -21, Steps Taken = 21, Time Elapsed = 1.84 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 3: Total Reward = -21, Steps Taken = 21, Time Elapsed = 1.42 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 4: Total Reward = -21, Steps Taken = 21, Time Elapsed = 1.44 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 5: Total Reward = -21, Steps Taken = 21, Time Elapsed = 1.38 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 6: Total Reward = -21, Steps Taken = 21, Time Elapsed = 1.41 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 7: Total Reward = -21, Steps Taken = 21, Time Elapsed = 1.41 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 8: Total Reward = -21, Steps Taken = 21, Time Elapsed = 1.35 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 9: Total Reward = -21, Steps Taken = 21, Time Elapsed = 1.70 seconds\n",
            "----------\n",
            "step: 1\n",
            "----------\n",
            "step: 2\n",
            "----------\n",
            "step: 3\n",
            "----------\n",
            "step: 4\n",
            "----------\n",
            "step: 5\n",
            "----------\n",
            "step: 6\n",
            "----------\n",
            "step: 7\n",
            "----------\n",
            "step: 8\n",
            "----------\n",
            "step: 9\n",
            "----------\n",
            "step: 10\n",
            "----------\n",
            "step: 11\n",
            "----------\n",
            "step: 12\n",
            "----------\n",
            "step: 13\n",
            "----------\n",
            "step: 14\n",
            "----------\n",
            "step: 15\n",
            "----------\n",
            "step: 16\n",
            "----------\n",
            "step: 17\n",
            "----------\n",
            "step: 18\n",
            "----------\n",
            "step: 19\n",
            "----------\n",
            "step: 20\n",
            "----------\n",
            "step: 21\n",
            "Too many negative points. Impossible to win. BYE!!!\n",
            "Episode 10: Total Reward = -21, Steps Taken = 21, Time Elapsed = 2.29 seconds\n"
          ]
        }
      ]
    }
  ]
}