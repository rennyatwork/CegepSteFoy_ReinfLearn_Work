{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk2V1Ui7GWRt5hufC7FW0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rennyatwork/CegepSteFoy_ReinfLearn_Work/blob/main/TP01/taxi_policy_gradient_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXe85Wma6rMp",
        "outputId": "c6b7a84d-d14f-4986-a388-f6d53e8ad7b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Am-DU5j76dHp"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make('Taxi-v3', render_mode='rgb_array')\n",
        "\n",
        "def decode_state(state):\n",
        "    \"\"\"Decode Taxi-v3 state into cell positions.\"\"\"\n",
        "    # Access the underlying TaxiEnv instance using 'unwrapped'\n",
        "    taxi_row, taxi_col, pass_loc, dest_idx = env.unwrapped.decode(state)\n",
        "    return f\"Taxi at ({taxi_row}, {taxi_col}), Passenger at {pass_loc}, Destination at {env.unwrapped.locs[dest_idx]}\"\n",
        "\n",
        "\n",
        "def policy_gradient(env, num_episodes=2000, learning_rate=0.01, discount_factor=0.95, pTheta=np.random.randn(env.observation_space.n, env.action_space.n) / np.sqrt(env.observation_space.n), pPrint=False):\n",
        "    \"\"\"Implement Policy Gradient (REINFORCE) for Taxi-v3.\"\"\"\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    theta = pTheta\n",
        "\n",
        "    def softmax(x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum()\n",
        "\n",
        "    def get_action(state, theta):\n",
        "        return np.random.choice(num_actions, p=softmax(theta[state]))\n",
        "\n",
        "    episode_times = []\n",
        "    iter_episode = []\n",
        "\n",
        "    initial_learning_rate = learning_rate\n",
        "    decay_rate = 1000  # Adjust for decay speed\n",
        "    beta = 0.01      # Entropy regularization strength (adjust as needed)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        start_time = time.time()\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        states, actions, rewards = [], [], []\n",
        "        penalties = 0\n",
        "        iter_count = 0\n",
        "        done = False\n",
        "\n",
        "        # Learning rate decay\n",
        "        learning_rate = initial_learning_rate / (1 + episode / decay_rate)\n",
        "\n",
        "        while not done:\n",
        "            action = get_action(state, theta)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            episode_reward += reward\n",
        "            iter_count += 1\n",
        "\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if episode_reward == -10:\n",
        "                penalties += 1\n",
        "                if penalties > 2:\n",
        "                    if pPrint:\n",
        "                        print(f'Too many penalties: {penalties}. BYE!!!')\n",
        "                    done = True\n",
        "\n",
        "            if iter_count > 60:\n",
        "                if pPrint:\n",
        "                    print(f'Too many iterations: {iter_count}. BYE!!!')\n",
        "                done = True\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Compute returns and apply baseline subtraction\n",
        "        G = 0\n",
        "        returns = []\n",
        "        for r in rewards[::-1]:\n",
        "            G = r + discount_factor * G\n",
        "            returns.insert(0, G)\n",
        "        baseline = np.mean(returns)\n",
        "        returns = [G - baseline for G in returns]\n",
        "\n",
        "        # Update theta with optional entropy regularization\n",
        "        for s, a, G in zip(states, actions, returns):\n",
        "            if G != 0:\n",
        "                prob_dist = softmax(theta[s])\n",
        "                theta[s] += learning_rate * (\n",
        "                    (G - np.mean(returns)) * (np.eye(num_actions)[a] - prob_dist)\n",
        "                    + beta * np.sum(prob_dist * np.log(prob_dist))  # Entropy term\n",
        "                )\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        episode_times.append(elapsed_time)\n",
        "        iter_episode.append(iter_count)\n",
        "\n",
        "        # Print episode statistics (same as before)\n",
        "        interval = int(num_episodes * 0.1)\n",
        "        if (episode + 1) % interval == 0 and episode != 0 and episode != num_episodes - 1:\n",
        "            avg_episode_time = np.mean(episode_times[-interval:])\n",
        "            print(f\"Episode {episode + 1}/{num_episodes} - \"\n",
        "                  f\"State: {decode_state(state)} - \" #Assuming you have defined `decode_state`\n",
        "                  f\"Final Action: {env.action_space.sample()} - \"\n",
        "                  f\"Average Return: {np.mean(returns):.3f}\"\n",
        "                  f\"Average Time per Episode: {avg_episode_time:.4f} seconds \"\n",
        "                  f'Avg iter/episode: {np.mean(iter_episode)}')\n",
        "\n",
        "    overall_avg_time = np.mean(episode_times)\n",
        "\n",
        "     # Print overall average time per episode\n",
        "    overall_avg_time = np.mean(episode_times)\n",
        "    print(f\"\\nOverall Average Time per Episode: {overall_avg_time:.3f} seconds\")\n",
        "    print(f\"\\nOverall Time per : {np.sum(episode_times):.3f} seconds\")\n",
        "\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate_policy(env, theta, num_episodes=100):\n",
        "    \"\"\"Evaluate the learned policy.\"\"\"\n",
        "    total_rewards = []\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        while True:\n",
        "            action = np.argmax(softmax(theta[state]))\n",
        "            state, reward, terminated, truncated, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        total_rewards.append(episode_reward)\n",
        "\n",
        "    return np.mean(total_rewards)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))  # For numerical stability\n",
        "    return e_x / e_x.sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3_tylXj_nDKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "learned_policy_01 = policy_gradient(env\n",
        "                                 ,  num_episodes=2000\n",
        "                                 , learning_rate=0.05\n",
        "                                 , discount_factor=0.97\n",
        "                                    , pPrint=False\n",
        "                                 )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H2yvruQ7_pp",
        "outputId": "4eacb0e4-4244-449b-d492-a8d458a0cfc2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 200/2000 - State: Taxi at (3, 0), Passenger at 1, Destination at (4, 0) - Final Action: 0 - Average Return: -0.000Average Time per Episode: 0.0082 seconds Avg iter/episode: 60.755\n",
            "Episode 400/2000 - State: Taxi at (4, 0), Passenger at 1, Destination at (4, 3) - Final Action: 4 - Average Return: -0.000Average Time per Episode: 0.0097 seconds Avg iter/episode: 60.7625\n",
            "Episode 600/2000 - State: Taxi at (4, 3), Passenger at 3, Destination at (0, 4) - Final Action: 0 - Average Return: -0.000Average Time per Episode: 0.0092 seconds Avg iter/episode: 60.84166666666667\n",
            "Episode 800/2000 - State: Taxi at (2, 3), Passenger at 3, Destination at (0, 0) - Final Action: 3 - Average Return: -0.000Average Time per Episode: 0.0081 seconds Avg iter/episode: 60.88125\n",
            "Episode 1000/2000 - State: Taxi at (1, 4), Passenger at 2, Destination at (0, 0) - Final Action: 0 - Average Return: 0.000Average Time per Episode: 0.0095 seconds Avg iter/episode: 60.868\n",
            "Episode 1200/2000 - State: Taxi at (0, 2), Passenger at 2, Destination at (4, 3) - Final Action: 4 - Average Return: -0.000Average Time per Episode: 0.0090 seconds Avg iter/episode: 60.89\n",
            "Episode 1400/2000 - State: Taxi at (3, 4), Passenger at 1, Destination at (0, 0) - Final Action: 5 - Average Return: -0.000Average Time per Episode: 0.0077 seconds Avg iter/episode: 60.90571428571429\n",
            "Episode 1600/2000 - State: Taxi at (3, 4), Passenger at 1, Destination at (0, 0) - Final Action: 0 - Average Return: -0.000Average Time per Episode: 0.0088 seconds Avg iter/episode: 60.9175\n",
            "Episode 1800/2000 - State: Taxi at (4, 2), Passenger at 0, Destination at (4, 3) - Final Action: 3 - Average Return: -0.000Average Time per Episode: 0.0094 seconds Avg iter/episode: 60.92666666666667\n",
            "\n",
            "Overall Average Time per Episode: 0.009 seconds\n",
            "\n",
            "Overall Time per : 17.793 seconds\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learned_policy_02 = policy_gradient(env\n",
        "                                 ,  num_episodes=10000\n",
        "                                 , learning_rate=0.05\n",
        "                                 , discount_factor=0.95\n",
        "                                    , pTheta=learned_policy_02\n",
        "                                    , pPrint=False\n",
        "                                 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wzv5GXvy9LLQ",
        "outputId": "b054f8b5-2325-4ce5-d001-e4a38c05e9c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - State: Taxi at (2, 3), Passenger at 3, Destination at (0, 4)\n",
            "  - Final Action: 2\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (4, 0), Passenger at 4, Destination at (0, 4)\n",
            "  - Final Action: 3\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (4, 3), Passenger at 1, Destination at (0, 0)\n",
            "  - Final Action: 0\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (4, 0), Passenger at 3, Destination at (0, 0)\n",
            "  - Final Action: 2\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (4, 3), Passenger at 3, Destination at (0, 0)\n",
            "  - Final Action: 5\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (1, 2), Passenger at 2, Destination at (4, 3)\n",
            "  - Final Action: 3\n",
            "  - Average Return: -18.14506660006586\n",
            "  - State: Taxi at (3, 2), Passenger at 3, Destination at (0, 0)\n",
            "  - Final Action: 4\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (3, 4), Passenger at 2, Destination at (4, 3)\n",
            "  - Final Action: 4\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 4), Passenger at 1, Destination at (4, 3)\n",
            "  - Final Action: 1\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 1), Passenger at 4, Destination at (4, 3)\n",
            "  - Final Action: 4\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 1), Passenger at 4, Destination at (4, 3)\n",
            "  - Final Action: 2\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (1, 2), Passenger at 2, Destination at (4, 3)\n",
            "  - Final Action: 2\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 4), Passenger at 1, Destination at (4, 3)\n",
            "  - Final Action: 0\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 4), Passenger at 1, Destination at (4, 3)\n",
            "  - Final Action: 0\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (1, 1), Passenger at 0, Destination at (0, 4)\n",
            "  - Final Action: 0\n",
            "  - Average Return: -181.0006660006586\n",
            "  - State: Taxi at (4, 2), Passenger at 2, Destination at (0, 4)\n",
            "  - Final Action: 5\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (4, 0), Passenger at 0, Destination at (4, 3)\n",
            "  - Final Action: 1\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (1, 2), Passenger at 2, Destination at (4, 3)\n",
            "  - Final Action: 0\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 4), Passenger at 1, Destination at (4, 3)\n",
            "  - Final Action: 3\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (4, 0), Passenger at 4, Destination at (0, 4)\n",
            "  - Final Action: 0\n",
            "  - Average Return: -18.187816600065858\n",
            "  - State: Taxi at (4, 0), Passenger at 4, Destination at (0, 4)\n",
            "  - Final Action: 0\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (4, 1), Passenger at 0, Destination at (4, 0)\n",
            "  - Final Action: 0\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (4, 2), Passenger at 3, Destination at (0, 0)\n",
            "  - Final Action: 1\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 0), Passenger at 0, Destination at (0, 0)\n",
            "  - Final Action: 4\n",
            "  - Average Return: 13.657956871093747\n",
            "  - State: Taxi at (1, 3), Passenger at 4, Destination at (4, 0)\n",
            "  - Final Action: 1\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (2, 3), Passenger at 3, Destination at (4, 0)\n",
            "  - Final Action: 3\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (1, 4), Passenger at 0, Destination at (0, 4)\n",
            "  - Final Action: 4\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (2, 4), Passenger at 0, Destination at (4, 0)\n",
            "  - Final Action: 4\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 2), Passenger at 2, Destination at (0, 0)\n",
            "  - Final Action: 5\n",
            "  - Average Return: -18.10006660006586\n",
            "  - State: Taxi at (0, 2), Passenger at 2, Destination at (0, 0)\n",
            "  - Final Action: 1\n",
            "  - Average Return: -18.10006660006586\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-00ec2b9a97e0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m learned_policy_02 = policy_gradient(env\n\u001b[0m\u001b[1;32m      2\u001b[0m                                  \u001b[0;34m,\u001b[0m  \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  \u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  \u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     \u001b[0;34m,\u001b[0m \u001b[0mpTheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearned_policy_02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a0d5c1db8680>\u001b[0m in \u001b[0;36mpolicy_gradient\u001b[0;34m(env, num_episodes, learning_rate, discount_factor, pTheta, pPrint)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m## next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_render_order_enforcing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_render_order_enforcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSupportsFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0;34m\"\"\"Steps through the environment.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_reward = evaluate_policy(env, learned_policy_02)\n",
        "print(f\"Average reward over 10 episodes: {avg_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxAx7Lcj6nqe",
        "outputId": "8841ab12-8dd8-4cba-e206-d21c620343a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward over 10 episodes: -229.54\n"
          ]
        }
      ]
    }
  ]
}