{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPObnGuRjjqRi2l7VmoPlmG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rennyatwork/CegepSteFoy_ReinfLearn_Work/blob/main/TP01/taxi_gradient_policy_ppo_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YhYII8ck0H8",
        "outputId": "c30083e6-a6a0-44af-bd45-92a548ab9c39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luffNUmQkWGt",
        "outputId": "57a50a24-d9a0-4dda-827c-809ab91ab73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 0.0%, Episode 1/10, Avg Reward: -830.00, Avg Steps: 200.00, Time: 2.4574s\n",
            "Progress: 10.0%, Episode 2/10, Avg Reward: -731.00, Avg Steps: 200.00, Time: 1.7077s\n",
            "Progress: 20.0%, Episode 3/10, Avg Reward: -812.00, Avg Steps: 200.00, Time: 2.2680s\n",
            "Progress: 30.0%, Episode 4/10, Avg Reward: -893.00, Avg Steps: 200.00, Time: 2.2569s\n",
            "Progress: 40.0%, Episode 5/10, Avg Reward: -785.00, Avg Steps: 200.00, Time: 1.6795s\n",
            "Progress: 50.0%, Episode 6/10, Avg Reward: -740.00, Avg Steps: 200.00, Time: 1.7190s\n",
            "Progress: 60.0%, Episode 7/10, Avg Reward: -866.00, Avg Steps: 200.00, Time: 1.8018s\n",
            "Progress: 70.0%, Episode 8/10, Avg Reward: -722.00, Avg Steps: 200.00, Time: 1.7912s\n",
            "Progress: 80.0%, Episode 9/10, Avg Reward: -812.00, Avg Steps: 200.00, Time: 1.6655s\n",
            "Progress: 90.0%, Episode 10/10, Avg Reward: -830.00, Avg Steps: 200.00, Time: 2.1011s\n",
            "Weights shapes from previous training: [(500, 32), (32,), (32, 6), (6,), (32, 1), (1,)]\n",
            "Model expected weights shapes: [TensorShape([500, 32]), TensorShape([32]), TensorShape([32, 6]), TensorShape([6]), TensorShape([32, 1]), TensorShape([1])]\n",
            "Progress: 0.0%, Episode 1/10, Avg Reward: -812.00, Avg Steps: 200.00, Time: 2.9875s\n",
            "Progress: 10.0%, Episode 2/10, Avg Reward: -767.00, Avg Steps: 200.00, Time: 2.4980s\n",
            "Progress: 20.0%, Episode 3/10, Avg Reward: -731.00, Avg Steps: 200.00, Time: 1.9510s\n",
            "Progress: 30.0%, Episode 4/10, Avg Reward: -830.00, Avg Steps: 200.00, Time: 1.6452s\n",
            "Progress: 40.0%, Episode 5/10, Avg Reward: -839.00, Avg Steps: 200.00, Time: 1.7126s\n",
            "Progress: 50.0%, Episode 6/10, Avg Reward: -812.00, Avg Steps: 200.00, Time: 1.9407s\n",
            "Progress: 60.0%, Episode 7/10, Avg Reward: -812.00, Avg Steps: 200.00, Time: 2.4100s\n",
            "Progress: 70.0%, Episode 8/10, Avg Reward: -794.00, Avg Steps: 200.00, Time: 1.5806s\n",
            "Progress: 80.0%, Episode 9/10, Avg Reward: -767.00, Avg Steps: 200.00, Time: 1.6501s\n",
            "Progress: 90.0%, Episode 10/10, Avg Reward: -821.00, Avg Steps: 200.00, Time: 1.7358s\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "class ActorCritic(tf.keras.Model):\n",
        "    def __init__(self, num_actions, hidden_units=64):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(hidden_units, activation=\"relu\")\n",
        "        self.policy = tf.keras.layers.Dense(num_actions, activation=\"softmax\")\n",
        "        self.value = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, input_data):\n",
        "        x = self.dense1(input_data)\n",
        "        return self.policy(x), self.value(x)\n",
        "\n",
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * next_value * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "        next_value = values[step]\n",
        "    return returns\n",
        "\n",
        "def ppo_loss(new_policy, old_policy, actions, advantages, clip_param=0.2):\n",
        "    prob_ratio = tf.exp(tf.math.log(tf.reduce_sum(new_policy * actions, axis=1) + 1e-10) -\n",
        "                        tf.math.log(tf.reduce_sum(old_policy * actions, axis=1) + 1e-10))\n",
        "    surr1 = prob_ratio * advantages\n",
        "    surr2 = tf.clip_by_value(prob_ratio, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
        "    return -tf.reduce_mean(tf.minimum(surr1, surr2))\n",
        "\n",
        "def train_ppo(env, num_episodes=2000, max_timesteps=1000, update_timesteps=2000, print_interval=0.1, weights=None, **kwargs):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=kwargs.get('learning_rate', 0.0003))\n",
        "\n",
        "    state_dim = env.observation_space.n\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    model = ActorCritic(action_dim, hidden_units=kwargs.get('hidden_units', 64))\n",
        "\n",
        "    # Build the model here to ensure weights are created\n",
        "    dummy_input = tf.keras.Input(shape=(state_dim,))\n",
        "    model(dummy_input)\n",
        "\n",
        "    if weights is not None:\n",
        "        print(\"Weights shapes from previous training:\", [w.shape for w in weights])\n",
        "        print(\"Model expected weights shapes:\", [w.shape for w in model.trainable_weights])\n",
        "        if len(weights) != len(model.trainable_weights):\n",
        "            raise ValueError(\"The number of weights provided does not match the model's expected weights.\")\n",
        "        model.set_weights(weights)\n",
        "\n",
        "\n",
        "    history = []\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = tf.one_hot(state, state_dim)\n",
        "        episode_start = time.time()\n",
        "\n",
        "        rewards = []\n",
        "        states, actions, values, log_probs, masks = [], [], [], [], []\n",
        "\n",
        "        for t in range(max_timesteps):\n",
        "            state_tensor = tf.expand_dims(state, 0)\n",
        "            with tf.GradientTape() as tape:\n",
        "                policy, value = model(state_tensor)\n",
        "                action_probs = tf.squeeze(policy)\n",
        "                value = tf.squeeze(value)\n",
        "                action = tf.random.categorical(tf.math.log([action_probs]), 1)[0, 0]\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action.numpy())\n",
        "            log_prob = tf.math.log(action_probs[action])\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(tf.one_hot(action, action_dim))\n",
        "            values.append(value)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            masks.append(1.0 - int(terminated or truncated))\n",
        "\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "            state = tf.one_hot(next_state, state_dim)\n",
        "\n",
        "        _, next_value = model(tf.one_hot(next_state, state_dim).numpy()[np.newaxis, :])\n",
        "        returns = compute_gae(next_value, rewards, masks, values)\n",
        "        advantages = np.array(returns) - np.array(values)\n",
        "        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-10)\n",
        "\n",
        "        old_policy, _ = model(tf.stack(states))\n",
        "        old_policy = tf.stop_gradient(old_policy)\n",
        "\n",
        "        for _ in range(10):  # PPO update iterations\n",
        "            with tf.GradientTape() as tape:\n",
        "                new_policy, _ = model(tf.stack(states))\n",
        "                loss = ppo_loss(new_policy, old_policy, tf.stack(actions), advantages)\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        total_reward = sum(rewards)\n",
        "        history.append((t+1, total_reward))\n",
        "\n",
        "        episode_time = time.time() - episode_start\n",
        "\n",
        "        if episode % int(num_episodes * print_interval) == 0:\n",
        "            interval = int(num_episodes * print_interval)\n",
        "            avg_reward = np.mean([reward for _, reward in history[-interval:]])\n",
        "            avg_steps = np.mean([steps for steps, _ in history[-interval:]])\n",
        "            print(f\"Progress: {episode/num_episodes*100:.1f}%, \"\n",
        "                  f\"Episode {episode+1}/{num_episodes}, \"\n",
        "                  f\"Avg Reward: {avg_reward:.2f}, \"\n",
        "                  f\"Avg Steps: {avg_steps:.2f}, \"\n",
        "                  f\"Time: {episode_time:.4f}s\")\n",
        "\n",
        "    return model.get_weights()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('Taxi-v3', render_mode='rgb_array')\n",
        "\n",
        "    # First training session\n",
        "    weights_1 = train_ppo(env, num_episodes=2, hidden_units=32, learning_rate=0.001)\n",
        "\n",
        "    # Second training session using weights from the first one\n",
        "    weights_2 = train_ppo(env, num_episodes=2, hidden_units=32, learning_rate=0.001, weights=weights_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Weights shapes from first training:\", [w.shape for w in weights_1])\n",
        "print(\"Model expected weights shapes:\", [w.shape for w in model.trainable_weights])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "G1vqnPFIqGXa",
        "outputId": "2d2e0f7f-6cad-4629-9053-bac5d179305d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights shapes from first training: [(500, 32), (32,), (32, 6), (6,), (32, 1), (1,)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-20512c9319da>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Weights shapes from first training:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model expected weights shapes:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " env = gym.make('Taxi-v3', render_mode='rgb_array')\n",
        "\n",
        "# First training session\n",
        "weights_1 = train_ppo(env, num_episodes=10, hidden_units=32, learning_rate=0.001)\n",
        "\n",
        "weights_2 = train_ppo(env, num_episodes=10, hidden_units=32, learning_rate=0.001, weights=weights_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "LFowZjhGkusz",
        "outputId": "af4d04ec-8c20-4fe0-eb82-263ad6c8509d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 0.0%, Episode 1/10, Avg Reward: -731.00, Avg Steps: 200.00, Time: 1.7672s\n",
            "Progress: 10.0%, Episode 2/10, Avg Reward: -830.00, Avg Steps: 200.00, Time: 1.7214s\n",
            "Progress: 20.0%, Episode 3/10, Avg Reward: -794.00, Avg Steps: 200.00, Time: 1.7665s\n",
            "Progress: 30.0%, Episode 4/10, Avg Reward: -794.00, Avg Steps: 200.00, Time: 2.1558s\n",
            "Progress: 40.0%, Episode 5/10, Avg Reward: -830.00, Avg Steps: 200.00, Time: 2.5112s\n",
            "Progress: 50.0%, Episode 6/10, Avg Reward: -839.00, Avg Steps: 200.00, Time: 1.8082s\n",
            "Progress: 60.0%, Episode 7/10, Avg Reward: -794.00, Avg Steps: 200.00, Time: 1.6851s\n",
            "Progress: 70.0%, Episode 8/10, Avg Reward: -803.00, Avg Steps: 200.00, Time: 1.6494s\n",
            "Progress: 80.0%, Episode 9/10, Avg Reward: -704.00, Avg Steps: 200.00, Time: 1.6908s\n",
            "Progress: 90.0%, Episode 10/10, Avg Reward: -884.00, Avg Steps: 200.00, Time: 1.7041s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "You called `set_weights(weights)` on layer 'actor_critic_12' with a weight list of length 6, but the layer was expecting 0 weights.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8fa71610cd06>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweights_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweights_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-5b4799fafd28>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(env, num_episodes, max_timesteps, update_timesteps, print_interval, weights, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now history will store tuples of (steps, total_reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlayer_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    678\u001b[0m                 \u001b[0;34mf\"You called `set_weights(weights)` on layer '{self.name}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;34mf\"with a weight list of length {len(weights)}, but the layer \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer 'actor_critic_12' with a weight list of length 6, but the layer was expecting 0 weights."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "_dPWtE68mJd3",
        "outputId": "3f058dfd-d8c8-4f92-dcef-316816286aaa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "You called `set_weights(weights)` on layer 'actor_critic_10' with a weight list of length 6, but the layer was expecting 0 weights.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9a3b48f68a52>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First training session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m weights_2 = train_ppo(env, num_episodes=10\n\u001b[0m\u001b[1;32m      3\u001b[0m                      \u001b[0;34m,\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                      \u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      , weights=weights_1)\n",
            "\u001b[0;32m<ipython-input-13-5b4799fafd28>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(env, num_episodes, max_timesteps, update_timesteps, print_interval, weights, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Now history will store tuples of (steps, total_reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlayer_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    678\u001b[0m                 \u001b[0;34mf\"You called `set_weights(weights)` on layer '{self.name}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;34mf\"with a weight list of length {len(weights)}, but the layer \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer 'actor_critic_10' with a weight list of length 6, but the layer was expecting 0 weights."
          ]
        }
      ]
    }
  ]
}