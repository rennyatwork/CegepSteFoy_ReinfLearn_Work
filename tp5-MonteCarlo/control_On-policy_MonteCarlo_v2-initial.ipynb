{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo On policy\n",
    "## Sources \n",
    "- Reinforcement Learning : An Introduction, by Richard Sutton and Andrew Barto\n",
    "\n",
    "Toujours basé sur le Blackjack, nous allons rechercher une politique optimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de l'environnement Blackjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionnaire pour stocker les Q values (action-valeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionnaire pour le total des retours G poour chaque couple s,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_return = defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionnaire pour le cumul du nombre de passage pour tout les s,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définissons une politique epsilon-greedy\n",
    "Paramètres d'entrés   \n",
    "    - Q fonction (s,a)   \n",
    "    - L'état courant   \n",
    "    - epsilon   \n",
    " \n",
    "Paramètre de sortie    \n",
    "    - a action choisie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state,Q,epsilon, pPrint=False):\n",
    "\n",
    "    # tirer un nombre aléatoire entre 0 et 1\n",
    "    # si ce nombre est inférieur à epsilon, tirer une action au hasard\n",
    "    # Sinon en partant de Q, pour l'état state, choisir l'action qui est la plus rémunératrice\n",
    "    # 10-15 lignes\n",
    "   \n",
    "    \n",
    "    random_nb = random.uniform(0,1)\n",
    "    if random_nb < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        max_q = -1\n",
    "        max_a = 0\n",
    "        for a in range(env.action_space.n):\n",
    "            if (state, a) in Q:\n",
    "                if Q[(state, a)] > max_q:\n",
    "                    max_q = Q[(state, a)] \n",
    "                    max_a = a\n",
    "        return max_a\n",
    "        \n",
    "    #return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy_2(state, Q, epsilon, pPrint=False):\n",
    "    # Draw a random number between 0 and 1\n",
    "    random_number = np.random.rand()\n",
    "\n",
    "    if pPrint:\n",
    "        print(f'random_number: {random_number}')\n",
    "        print(f'epsilon: {epsilon}')\n",
    "        print(f'Q: {Q}')\n",
    "        print(f'state: {state}')\n",
    "        \n",
    "    # Filter the actions for the given state\n",
    "    actions_for_state = {action: Q[(state, action)] for action in range(len(Q)) if (state, action) in Q}\n",
    "    \n",
    "    if pPrint:\n",
    "        print(f'actions_for_state: {actions_for_state}')\n",
    "    \n",
    "    # If the random number is less than epsilon, choose a random action (exploration)\n",
    "    if random_number < epsilon:\n",
    "        action = np.random.choice(list(actions_for_state.keys()))  # Random action from the action space\n",
    "        \n",
    "        if pPrint:\n",
    "            print(f'[IF] Action: {action}')\n",
    "        \n",
    "    else:\n",
    "        # Otherwise, choose the action with the highest Q value (exploitation)\n",
    "        action = max(actions_for_state, key=actions_for_state.get)  # Select the action with the maximum Q value\n",
    "        if pPrint:\n",
    "            print(f'[ELSE] Action: {action}')\n",
    "    \n",
    "    return action  # Return the chosen action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 1\n"
     ]
    }
   ],
   "source": [
    "Z=defaultdict(float)\n",
    "Z[(1,0)] = 10\n",
    "Z[(1,1)] = 12\n",
    "action = epsilon_greedy_policy(1,Z,0, pPrint=True) \n",
    "print (f'action: {action}')\n",
    "#assert epsilon_greedy_policy(1,Z,0) ==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {(1, 0): 10, (1, 1): 12, 1: 0.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max key: (1, 1), Max value: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_key = max(Z, key=Z.get)\n",
    "max_value = Z[max_key]\n",
    "print(f\"Max key: {max_key}, Max value: {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: (1, 0)\n",
      "k: (1, 1)\n",
      "k: 1\n"
     ]
    }
   ],
   "source": [
    "for k in Z:\n",
    "    print(f'k: {k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Générateur d'épisode\n",
    "Entrée :\n",
    "- Q fonction\n",
    "- espilon pour notre politique aléatoire\n",
    "\n",
    "Sortie :\n",
    "- notre épisode sous la forma s,a ,s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 100\n",
    "#epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(Q,epsilon):\n",
    "    \n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        # Sélection d'une action en fonction de notre politique\n",
    "        action = epsilon_greedy_policy(state,Q,epsilon)\n",
    "        \n",
    "        # envoie de l'action à l'environnement pour retour (s_, r, done)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # stockage dans la liste du triplet (état, action, récompense)\n",
    "        episode.append((state, action, reward))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Calcul de la politique optimale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_iterations = 50000\n",
    "num_iterations = 3\n",
    "espilon = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons initialiser une politique random au démarrage.      \n",
    "Selon le modèle GPI, Notre Q fonction va tendre vers l'optimale et donc notre politique également.<br>\n",
    "Pour simplifier et parce que l'intéret est limité dans le cas du blackjack, nous ferons abstraction de gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "for i in range(num_iterations ):\n",
    "    \n",
    "    # on génére un épisode\n",
    "    episode = generate_episode(Q,espilon)\n",
    "    \n",
    "    # on stocker les pairs s,a de l'épisode\n",
    "    all_state_action_pairs = [(s, a) for (s,a,r) in episode]\n",
    "    \n",
    "    # on stocke les récompense\n",
    "    rewards = [r for (s,a,r) in episode]\n",
    "\n",
    "    # Pour chaque t de l'épisode \n",
    "    for t, (state, action, reward) in enumerate(episode):\n",
    "\n",
    "        # First visit : on ne prend en compte que le premier passage s,a\n",
    "        if not (state, action) in all_state_action_pairs[0:t]:\n",
    "            \n",
    "            # Calcul de G avec y = 1\n",
    "            G = sum(rewards[t:]) \n",
    "            \n",
    "            # Accumulate G for the (state, action) pair\n",
    "            total_return[(state, action)] = total_return.get((state, action), 0) + G\n",
    "            \n",
    "            \n",
    "            # Comptage du nombre de passage\n",
    "            N[(state, action)] = N.get((state, action), 0) + 1\n",
    "\n",
    "            # Calcul de Q value (s,a) par la moyenne des G cumulés sur N\n",
    "            # Votre code : mettre à jour Q, ce qui mettrait à jour notre politique\n",
    "            # 1 ligne\n",
    "             # Update the Q-value (state, action) as the average of cumulative returns            \n",
    "            Q[(state, action)] = total_return[(state, action)] / N[(state, action)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stockage du résultat dans un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Q.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(Q.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(Q.items(),columns=['state_action pair','value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_action pair</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((8, 5, 0), 0)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((18, 2, 0), 0)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((11, 3, 0), 0)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((16, 10, 0), 0)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((19, 7, 0), 0)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((19, 7, 0), 1)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((17, 4, 0), 0)</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state_action pair  value\n",
       "0    ((8, 5, 0), 0)   -1.0\n",
       "1   ((18, 2, 0), 0)    1.0\n",
       "2   ((11, 3, 0), 0)   -1.0\n",
       "3  ((16, 10, 0), 0)   -1.0\n",
       "4   ((19, 7, 0), 0)    1.0\n",
       "5   ((19, 7, 0), 1)    0.0\n",
       "6   ((17, 4, 0), 0)   -1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons un Q optimal ou proche.   \n",
    "Notre politique est basée sur une approche gloutonne de Q donc elle même optimale !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exemple\n",
    "J'ai en main 19 points (sans as) et le croupier 5 points visibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_action pair</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>((19, 5, 0), 0)</td>\n",
       "      <td>0.467101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state_action pair     value\n",
       "226   ((19, 5, 0), 0)  0.467101"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ df['state_action pair'] == ((19,5,False),0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_action pair</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>((10, 10, 0), 1)</td>\n",
       "      <td>-0.220868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state_action pair     value\n",
       "207  ((10, 10, 0), 1) -0.220868"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ df['state_action pair'] == ((10,10,False),1) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ma politique va donc choisir un stand (0) car la valeur de fonction d'action-état est la plus haute !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon_greedy_policy((10, 10, 0), Q, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
