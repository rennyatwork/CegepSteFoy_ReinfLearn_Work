{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIiDGP56AdfVUFEUrAFiFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rennyatwork/CegepSteFoy_ReinfLearn_Work/blob/main/TD01/taxi_dqn_v02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "#!pip install segment_tree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-QwruIujyHa",
        "outputId": "2abf3aa9-61ca-46f8-d342-1154a27c1a10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LdkW6STLjnBL"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Environment Setup\n",
        "env = gym.make('Taxi-v3', render_mode=\"rgb_array\")\n",
        "state_size = env.observation_space.n  # Taxi-v3 has 500 states\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# DQN Model\n",
        "class DQNModel:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.model = self._build_model(state_size, action_size)\n",
        "        self.target_model = tf.keras.models.clone_model(self.model)\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def _build_model(self, state_size, action_size):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(128, activation='relu', input_shape=(state_size,)),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      loss='mse')\n",
        "        return model\n",
        "\n",
        "    def predict(self, state):\n",
        "        print('[BEGIN predict]')\n",
        "        #print(f'[state] : {state} ')\n",
        "        # Ensure state is passed as a numpy array with the correct dimensions\n",
        "        if np.isscalar(state):\n",
        "            state = np.eye(state_size)[state]  # Convert scalar to one-hot encoded array\n",
        "        else:\n",
        "            assert state.shape == (state_size,), f\"Input shape mismatch, got {state.shape}, expected {(state_size,)}\"\n",
        "        print('[END predict]')\n",
        "        #return self.model.predict(np.array([state]), verbose = 0)[0]\n",
        "        return self.model.__call__(np.array([state]))[0]\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.005\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = DQNModel(state_size, action_size)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        print('[BEGIN update_target_model]')\n",
        "        self.model.update_target_model()\n",
        "        print('[END update_target_model]')\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done, pPrint=False):\n",
        "        print('[BEGIN remember]')\n",
        "        if pPrint:\n",
        "\n",
        "            print(f'[state]: {state}')\n",
        "            print(f'[action]: {action}')\n",
        "            print(f'[reward]: {reward}')\n",
        "            print(f'[next_state]: {next_state}')\n",
        "            print(f'[done]: {done}')\n",
        "        state = np.eye(self.state_size)[state] if isinstance(state, int) else state\n",
        "        next_state = np.eye(self.state_size)[next_state] if isinstance(next_state, int) else next_state\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        print('[END remember]')\n",
        "\n",
        "    ### policy epsilon-greedy\n",
        "    def act(self, state):\n",
        "        print('[BEGIN act (policy)]')\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        #act_values = self.model.predict(state)\n",
        "        act_values = self.model.__call__(state)\n",
        "        print('[END act]')\n",
        "        return np.argmax(act_values)\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "      print('[BEGIN replay]')\n",
        "      minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "      for state, action, reward, next_state, done in minibatch:\n",
        "          #target = self.model.predict(state)\n",
        "          target = self.model.__call__(state)\n",
        "\n",
        "          if done:\n",
        "              target[action] = reward\n",
        "          else:\n",
        "              # Double DQN Logic:\n",
        "              # 1. Use online network to select the best action in the next state\n",
        "              best_action = np.argmax(self.model.predict(next_state))\n",
        "              # 2. Use target network to estimate the Q-value of that best action\n",
        "              #target[action] = reward + self.gamma * self.model.target_model.predict(next_state)[0][best_action]\n",
        "              target[action] = reward + self.gamma * self.model.target_model.__call__(next_state)[0][best_action]\n",
        "\n",
        "          # Update the model for the current batch\n",
        "          self.model.model.fit(np.array([state]), np.array([target]), epochs=1, verbose=0)\n",
        "\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "          self.epsilon *= self.epsilon_decay\n",
        "      print('[END replay]')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "      print('[BEGIN replay]')\n",
        "      minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
        "      for state, action, reward, next_state, done in minibatch:\n",
        "          #target = self.model.predict(state)  # Predict Q-values for the current state\n",
        "          target = self.model.__call__(state)  # Predict Q-values for the current state\n",
        "\n",
        "          if done:\n",
        "              target[action] = reward  # If done, target is just the reward\n",
        "          else:\n",
        "              # Double DQN Logic:\n",
        "              # 1. Use online network to select the best action in the next state\n",
        "              #best_action = np.argmax(self.model.predict(next_state))\n",
        "              best_action = np.argmax(self.model.__call__(next_state))\n",
        "              # 2. Use target network to estimate the Q-value of that best action\n",
        "              next_state_reshaped = next_state.reshape(1, -1)  # Reshape next_state for target_model\n",
        "              #target[action] = reward + self.gamma * self.model.target_model.predict(next_state_reshaped, verbose=0)[0][best_action]\n",
        "              target[action] = reward + self.gamma * self.model.target_model.__call__(next_state_reshaped, verbose=0)[0][best_action]\n",
        "\n",
        "          # Update the model for the current batch\n",
        "          state_reshaped = state.reshape(1, -1)  # Reshape state before fitting\n",
        "          self.model.model.fit(state_reshaped, np.array([target]), epochs=1, verbose=0)\n",
        "\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "          self.epsilon *= self.epsilon_decay\n",
        "      print('[END replay]')\n",
        "\n",
        "\n",
        "    def train(self, episodes, batch_size):\n",
        "        print('[BEGIN train]')\n",
        "        scores = []\n",
        "        for e in range(episodes):\n",
        "            print('')\n",
        "            print(f'----------------------------------------- ')\n",
        "            print(f'Episode {e} ')\n",
        "            state = env.reset()\n",
        "            state = state[0] if isinstance(state, tuple) else state  # Handle tuple output if it's possible in newer gym versions\n",
        "            done = False\n",
        "            score = 0\n",
        "\n",
        "            iter=0\n",
        "            while not done and iter <120:\n",
        "                print(f'iter: {iter}')\n",
        "                iter+=1\n",
        "\n",
        "                action = self.act(state)\n",
        "                next_state, reward, terminated, truncated, info = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                reward = reward if not done else -10  # Penalize for not solving the environment\n",
        "\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                score += reward\n",
        "\n",
        "                print(f'[epidosde] {e} - [iter] {iter}  - [ score ] : {score}')\n",
        "                if done :\n",
        "                    scores.append(score)\n",
        "                    print(f\"Episode: {e}/{episodes}, Score: {score}, Epsilon: {self.epsilon:.2}\")\n",
        "                    if e % 10 == 0:\n",
        "                        self.update_target_model()\n",
        "                    break\n",
        "\n",
        "                # Call replay every 7 iterations instead of every action\n",
        "                if len(self.memory) > batch_size and iter % 7 == 0:\n",
        "                    self.replay(batch_size)\n",
        "        print('[END train]')\n",
        "\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the agent\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "scores = agent.train(500, batch_size=32)\n",
        "\n",
        "# Plotting the results\n",
        "plt.plot(scores)\n",
        "plt.title('Training Scores over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5mIHE1ZZkFaJ",
        "outputId": "202bde69-69f1-4622-fc24-cca4841a398a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BEGIN update_target_model]\n",
            "[END update_target_model]\n",
            "[BEGIN train]\n",
            "\n",
            "----------------------------------------- \n",
            "Episode 0 \n",
            "iter: 0\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 1  - [ score ] : -1\n",
            "iter: 1\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 2  - [ score ] : -11\n",
            "iter: 2\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 3  - [ score ] : -21\n",
            "iter: 3\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 4  - [ score ] : -22\n",
            "iter: 4\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 5  - [ score ] : -23\n",
            "iter: 5\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 6  - [ score ] : -24\n",
            "iter: 6\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 7  - [ score ] : -34\n",
            "iter: 7\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 8  - [ score ] : -35\n",
            "iter: 8\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 9  - [ score ] : -45\n",
            "iter: 9\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 10  - [ score ] : -46\n",
            "iter: 10\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 11  - [ score ] : -47\n",
            "iter: 11\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 12  - [ score ] : -57\n",
            "iter: 12\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 13  - [ score ] : -67\n",
            "iter: 13\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 14  - [ score ] : -77\n",
            "iter: 14\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 15  - [ score ] : -78\n",
            "iter: 15\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 16  - [ score ] : -88\n",
            "iter: 16\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 17  - [ score ] : -89\n",
            "iter: 17\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 18  - [ score ] : -99\n",
            "iter: 18\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 19  - [ score ] : -100\n",
            "iter: 19\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 20  - [ score ] : -110\n",
            "iter: 20\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 21  - [ score ] : -111\n",
            "iter: 21\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 22  - [ score ] : -121\n",
            "iter: 22\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 23  - [ score ] : -122\n",
            "iter: 23\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 24  - [ score ] : -123\n",
            "iter: 24\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 25  - [ score ] : -133\n",
            "iter: 25\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 26  - [ score ] : -143\n",
            "iter: 26\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 27  - [ score ] : -144\n",
            "iter: 27\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 28  - [ score ] : -145\n",
            "iter: 28\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 29  - [ score ] : -146\n",
            "iter: 29\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 30  - [ score ] : -147\n",
            "iter: 30\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 31  - [ score ] : -148\n",
            "iter: 31\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 32  - [ score ] : -149\n",
            "iter: 32\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 33  - [ score ] : -150\n",
            "iter: 33\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 34  - [ score ] : -151\n",
            "iter: 34\n",
            "[BEGIN act (policy)]\n",
            "[BEGIN remember]\n",
            "[END remember]\n",
            "[epidosde] 0 - [iter] 35  - [ score ] : -152\n",
            "[BEGIN replay]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DQNModel' object has no attribute '__call__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1ab74df39a6d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plotting the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1f483ea29c59>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes, batch_size)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;31m# Call replay every 7 iterations instead of every action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[END train]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1f483ea29c59>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m           \u001b[0;31m#target = self.model.predict(state)  # Predict Q-values for the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m           \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Predict Q-values for the current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DQNModel' object has no attribute '__call__'"
          ]
        }
      ]
    }
  ]
}