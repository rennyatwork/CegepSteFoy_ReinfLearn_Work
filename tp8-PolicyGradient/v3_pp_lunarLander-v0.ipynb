{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7664c3d8-a96e-422e-ba97-7139542f4af1",
   "metadata": {},
   "source": [
    "# Lunar Lander !\n",
    "\n",
    "## Policy gradient\n",
    "\n",
    "(c) Fabrice Mulotti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7aba2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current path: /home/renato/Git_Ia/ReinforcementLearnig/reinf_learn_work_copy/tp8-PolicyGradient\n",
      "Virtual environment path: /home/renato/PythonVenvs/venvPolicyGrd\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_path = Path.cwd()\n",
    "print(\"Current path:\", current_path)\n",
    "venv_path = Path(sys.prefix)\n",
    "print(\"Virtual environment path:\", venv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943c575c-ae33-4d63-a56d-4ac3b7053030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../lib\")\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6b1cc2d-fbc8-4f25-9394-bdea24726fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 21:08:27.503651: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4008e-c8b7-4ea1-b2b4-c1fa025bfe0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e8eae5c-5219-4c01-b303-5afd4e8dc772",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient():\n",
    "    def __init__(self,nb_obs,nb_action,learning_rate=0.001,gamma=0.99, layer1=64,layer2=32):\n",
    "        self.nb_obs=nb_obs\n",
    "        self.nb_action=nb_action\n",
    "        self.learning_rate=learning_rate\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # création du réseau\n",
    "        self.policy, self.predict = self.buildNN(layer1,layer2)\n",
    "        \n",
    "        # mémoire des transitions\n",
    "        self.histo_states = []\n",
    "        self.histo_actions =[]\n",
    "        self.histo_rewards = []\n",
    "        \n",
    "\n",
    "        \n",
    "    def buildNN(self,layer1,layer2):\n",
    "        \"\"\"\n",
    "        Objet : création du réseau neural\n",
    "        entrée : couches cachées 1 et 2\n",
    "        sortie : réseau pour entrainement et prédiction\n",
    "        \"\"\"\n",
    "\n",
    "        InputReward = Input(shape=[1])\n",
    "        InputOBS    = Input(shape=(self.nb_obs,))\n",
    "\n",
    "         # Hidden layers with ReLU activation\n",
    "        dense1 = Dense(layer1, activation='relu', name=\"Dense1\")(InputOBS)\n",
    "        dense2 = Dense(layer2, activation='relu', name=\"Dense2\")(dense1)\n",
    "\n",
    "        # Output layer with softmax activation for probability output\n",
    "        proba = Dense(self.nb_actions, activation='softmax', name=\"OutputProba\")(dense2)\n",
    "\n",
    "\n",
    "        # Model for training (with reward as input)\n",
    "        policy = Model(inputs=[InputOBS, InputReward], outputs=[proba])\n",
    "        policy.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=self.loss_function)\n",
    "\n",
    "        # Model for prediction (without reward as input)\n",
    "        predict = Model(inputs=[InputOBS], outputs=[proba])\n",
    "\n",
    "        return policy, predict\n",
    "    \n",
    "\n",
    "        \n",
    "        def loss_function(y_true, y_pred):\n",
    "            \"\"\"\n",
    "            Objet : fonction de perte\n",
    "            Entrée : y réel et y prédit\n",
    "            Sortie : loss\n",
    "            \"\"\"\n",
    "            out = tf.keras.backend.clip(y_pred,1e-8,1-1e-8) # on élimine les valeurs extrèmes\n",
    "            loss_brut = y_true * tf.keras.backend.log(out)\n",
    "            loss_reward = -loss_brut * InputReward\n",
    "            print(\"Loss reward \",loss_reward)\n",
    "            return tf.keras.backend.sum(loss_reward)\n",
    "    \n",
    "        policy=Model(inputs=[InputOBS,InputReward], outputs=[proba])\n",
    "        policy.compile(optimizer=Adam(learning_rate=self.learning_rate),loss=loss_function)\n",
    "        predict=Model(inputs=[InputOBS], outputs=[proba])\n",
    "        \n",
    "        return policy,predict\n",
    "                       \n",
    "    def act(self,state):\n",
    "        \"\"\"\n",
    "        Objet : choisi une action en fonction d'un état et de la distribution des prob\n",
    "        Entrée : état\n",
    "        Sortie : action\n",
    "        \"\"\"\n",
    "        # Votre code\n",
    "         # Get the action probabilities from the model's prediction\n",
    "        action_probabilities = self.policy.predict(np.array([state]))[0]\n",
    "        \n",
    "        # Choose an action based on the probabilities (using np.random.choice)\n",
    "        action = np.random.choice(len(action_probabilities), p=action_probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def discount_reward(self,histo_reward):\n",
    "        \"\"\"\n",
    "        Objet : renvoie le gain d'une trajectoire \n",
    "        Entrée : historique des récompenses pour chaque transition \n",
    "        Sortie : Gain avec dépréciation gamma : np.array pour tracer le gain à chaque step\n",
    "        \"\"\"\n",
    "        # creation d un array a zero memes dimensions que rewards\n",
    "        discounted = np.zeros_like(histo_reward)\n",
    "        # votre code\n",
    "         # Create an array of zeros with the same shape as the input rewards\n",
    "        discounted = np.zeros_like(histo_reward, dtype=np.float32)\n",
    "        \n",
    "        # Initialize the cumulative reward\n",
    "        cumulative_reward = 0.0\n",
    "        \n",
    "        # Calculate the discounted reward for each step in reverse order\n",
    "        for t in reversed(range(len(histo_reward))):\n",
    "            cumulative_reward = histo_reward[t] + self.gamma * cumulative_reward\n",
    "            discounted[t] = cumulative_reward\n",
    "        \n",
    "        return discounted\n",
    "    \n",
    "    def memory(self,state,action,reward):\n",
    "        \"\"\"\n",
    "        Objet : historisation des transitions\n",
    "        Entrée : état, action, récompense\n",
    "        Sortie : none\n",
    "        \"\"\"\n",
    "        self.histo_states.append(state[0])\n",
    "        self.histo_actions.append(action)\n",
    "        self.histo_rewards.append(reward)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Objet : entrainement du model en fonction de la dernière trajectoire\n",
    "        \"\"\"\n",
    "        states_history = np.array(self.histo_states)\n",
    "        actions_history = np.array(self.histo_actions)\n",
    "        rewards_history = np.array(self.histo_rewards)\n",
    "\n",
    "        # transformation de l'historique des action en matrice 0/1\n",
    "        actions=np.zeros([len(actions_history),self.nb_action])\n",
    "        actions[ np.arange(len(actions_history)), actions_history] = 1\n",
    "\n",
    "        # calcul du Gain normalisé\n",
    "        discounted_reward = self.discount_reward(rewards_history)\n",
    "        discounted_reward -= np.mean(discounted_reward)\n",
    "        discounted_reward /= np.std(discounted_reward)\n",
    "        # print(discounted_reward.shape)\n",
    "        \n",
    "        # entrainement\n",
    "        self.policy.train_on_batch([ states_history, discounted_reward ] , actions)\n",
    "        self.histo_states = []\n",
    "        self.histo_actions =[]\n",
    "        self.histo_rewards = []\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db080259-69df-4a16-85b5-82912261d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation des données pour compatabilité avec l'alimentation du réseau de neurones (1,s)`\n",
    "def trans_state(s):\n",
    "    return  np.reshape(s, [1, nb_obs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d3a8f-a89b-42d7-b919-3270380aa8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graf=tools.Grafana(\"/var/www/html/files/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79def83-a5f1-429a-a7a9-c96083a884bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de carateristiques des états : 8, nombre d actions possibles : 4\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('LunarLander-v3')\n",
    "nb_obs=env.observation_space.shape[0]\n",
    "nb_action=env.action_space.n\n",
    "print(f\"Nombre de carateristiques des états : {nb_obs}, nombre d actions possibles : {nb_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f20e069-063f-4739-baf0-cb1cf01102cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PolicyGradient' object has no attribute 'nb_actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent\u001b[38;5;241m=\u001b[39m\u001b[43mPolicyGradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnb_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlayer2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 0.98 l2 96\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mPolicyGradient.__init__\u001b[0;34m(self, nb_obs, nb_action, learning_rate, gamma, layer1, layer2)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m=\u001b[39mgamma\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# création du réseau\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuildNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# mémoire des transitions\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhisto_states \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m, in \u001b[0;36mPolicyGradient.buildNN\u001b[0;34m(self, layer1, layer2)\u001b[0m\n\u001b[1;32m     30\u001b[0m dense2 \u001b[38;5;241m=\u001b[39m Dense(layer2, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDense2\u001b[39m\u001b[38;5;124m\"\u001b[39m)(dense1)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Output layer with softmax activation for probability output\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m proba \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_actions\u001b[49m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputProba\u001b[39m\u001b[38;5;124m\"\u001b[39m)(dense2)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Model for training (with reward as input)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m policy \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[InputOBS, InputReward], outputs\u001b[38;5;241m=\u001b[39m[proba])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PolicyGradient' object has no attribute 'nb_actions'"
     ]
    }
   ],
   "source": [
    "agent=PolicyGradient(nb_obs,nb_action, learning_rate=0.0001, gamma=0.99,layer1=64,layer2=32 ) # 0.98 l2 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb786483-874d-499d-9f52-0dfaee19e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=os.path.join(\"poids\",\"pp_lunar_save_weights.hp5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e2de6-2a20-4be4-b42a-15766ae1f0f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_episodes = 20000\n",
    "time_step = 0  # comptage du nombre total de mouvement\n",
    "histoReturn=[] # pour graphique sur historique récompense\n",
    "\n",
    "seuilWin=180\n",
    "win=False\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # somme de la récompense total pour une cycle\n",
    "    Return = 0\n",
    "    \n",
    "    # reset env et conversion state\n",
    "    state = trans_state(env.reset()[0])\n",
    "    done=False\n",
    "    truncated=False\n",
    "    startTime=time.time()\n",
    "    moveCount=0\n",
    "    while not(done or truncated):\n",
    "        \n",
    "        # décommenter pour affichage\n",
    "        # env.render()\n",
    "    \n",
    "        time_step += 1\n",
    "        moveCount += 1\n",
    "                \n",
    "        # sélection d'une action selon notre politique\n",
    "        action = agent.act(state)\n",
    " \n",
    "        # jouer l'action\n",
    "        next_state, reward, done, truncated , _ = env.step(action)\n",
    "        next_state=trans_state(next_state) # reformatage\n",
    "\n",
    "        # on met en memoire\n",
    "        agent.memory(state,action,reward)\n",
    "        \n",
    "        # et shift d'état\n",
    "        state = trans_state(next_state)\n",
    "        \n",
    "        # cumul du retour G\n",
    "        Return += reward\n",
    "\n",
    "       \n",
    "        # Done ?\n",
    "        if done or truncated:\n",
    "            # affichage du résultat du cyle\n",
    "            duration=time.time()-startTime\n",
    "            print('Episode: ',i, ',' 'Return', np.round(Return,2),', durée ',np.round(duration,2),' seconde en ',moveCount, ' mouvements, end=',truncated)\n",
    "            # graf.add_record(i,Return,0,moveCount,duration)\n",
    "            if np.mean(histoReturn[-5:]) > seuilWin:\n",
    "                print(\"Yeah ! Gagné !\")\n",
    "                win=True\n",
    "                \n",
    "            histoReturn.append(Return)\n",
    "\n",
    "            agent.train()\n",
    "            \n",
    "            # Sauvegarde des poids tous les 50 cycles\n",
    "            #if i % 50 == 0:\n",
    "            #    agent.policy.save_weights(checkpoint_path)\n",
    "                \n",
    "            break\n",
    "    if win:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1785b-cdff-4fbd-95c3-0618284a0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(histoReturn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399596f-f906-4d45-b5da-cdad7a304791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvPolicyGrd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
